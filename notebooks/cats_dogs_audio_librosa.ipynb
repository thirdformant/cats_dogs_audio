{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_dogs_audio_librosa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thirdformant/cats_dogs_audio/blob/master/notebooks/cats_dogs_audio_librosa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yNa0zvIFtgZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Audio classification of cats and dogs"
      ]
    },
    {
      "metadata": {
        "id": "hRdNgGMetuHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training a machine learning classification model on images of cats and dogs is a common introductory problem. With modern convolutional neural network (CNN) architectures and transfer learning, it is now possible to achieve near-perfect levels of classification accuracy as demonstrated very effectively (and efficiently!) early on in the [fast.ai deep learning MOOC](https://course.fast.ai/). When browsing through the datasets on Kaggle some time ago, I stumbled upon the [Audio Cats and Dogs](https://www.kaggle.com/mmoreaux/audio-cats-and-dogs/home) data, which presents itself as 'the audio counterpart' to the typical image classification problem. As someone with a background in acoustic phonetics, this has rather a lot of appeal. In addition, the dataset brings some new challenges to classification:  first, the audio data requires considerable preprocessing if a CNN is to be used, and second, the data consists of only 277 files. More on both these topics later.\n",
        "\n",
        "This notebook comprises two major parts. The first is a partial implementation of the approaches described in Huzaifah (2017) in his experimentation regarding CNN-based classification of environmental sound data. While the CNN architectures he used were implemented in TensorFlow, I have chosen instead to use Pytorch. The second section ((improves on the approaches used by Huzaifah, increasing the overall classification accuracy using TODO: finish this once the implementation is done))."
      ]
    },
    {
      "metadata": {
        "id": "Pjyov0JX10vr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries and setup"
      ]
    },
    {
      "metadata": {
        "id": "-tJQoLFbc2ld",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Librosa](http://librosa.github.io/librosa/) is a general-purpose audio processing and analysis library."
      ]
    },
    {
      "metadata": {
        "id": "oCMQApJuiRft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install sox libsox-dev libsox-fmt-all\n",
        "# !pip3 install git+git://github.com/pytorch/audio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lDu3Ygur2Mu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import librosa # Audio library\n",
        "\n",
        "# Data viz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "# import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# transforms\n",
        "import torchvision.transforms as transforms\n",
        "# import torchaudio.transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6PjxjQJBwFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXP80igW8tkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Connecting to google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eqsk_vjC2oGJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reading audio data"
      ]
    },
    {
      "metadata": {
        "id": "vzTyjOv58Zs6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I created a csv file containing the filename and the file labels. This can be passed to the Pytorch Dataset class defined in the next section."
      ]
    },
    {
      "metadata": {
        "id": "PpkduA0421es",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "ROOT_PATH = Path('/content/gdrive/My Drive/data/cats_dogs_audio/cats_dogs/')\n",
        "CSV_PATH = Path('/content/gdrive/My Drive/data/cats_dogs_audio/all_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3vzPD50qjsy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Creates the csv file. Commented out because I only needed to run it once\n",
        "\n",
        "# all_data = {\n",
        "#     'label': [],\n",
        "#     'filename': []\n",
        "# }\n",
        "\n",
        "\n",
        "# files_list = os.listdir(ROOT_PATH)\n",
        "# for f in files_list:\n",
        "#     if Path(f).suffix == '.wav':\n",
        "#         all_data['filename'].append(f)\n",
        "#         if 'cat' in f:\n",
        "#             all_data['label'].append(0)\n",
        "#         else:\n",
        "#             all_data['label'].append(1)\n",
        "\n",
        "# data_df = pd.DataFrame(all_data).iloc[:, ::-1] # Reverse the column order\n",
        "# data_df.to_csv(CSV_PATH, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLhW72lJ-PO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(CSV_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVGFQB_AwArl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(f'The distribution of classes in the data is: {Counter(data_df[\"label\"])}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMnU7-ElJ3Jt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "metadata": {
        "id": "dDBHyFN3-TTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Waveforms show changes in the signal, especially in its amplitude (loudness), over time and are therefore representations of the signal in the *time domain*. This is, however, only one aspect of the signal. When it comes to training audio classification or e.g. speech recognition models, better results are achieved when considering how the frequency of the signal, rather than its amplitude, changes over time. These are known as *time-frequency* representations, the best-known of which is the spectrogram.\n",
        "\n",
        "While Huzaifah (2017) created four time-frequency representations, only two are currently implemented here:\n",
        "- Linear-scaled Short-time Fourier Transform (STFT) spectrograms\n",
        "- mel-scaled STFT spectrograms.\n",
        "\n",
        "In addition, Mel-frequency Cepstral Coefficients (MFCCs) were also extracted from the input signal.\n",
        "\n",
        "Prior to feature extractions, all raw audio was either clipped or padded to a 4 second duration."
      ]
    },
    {
      "metadata": {
        "id": "tHz754Fy--eo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pytorch dataset definition"
      ]
    },
    {
      "metadata": {
        "id": "VeB86ajfYQXS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pytorch datasets allow the audio features to be extracted and transformed 'on the fly' when needed, rather than being extracted in advance and saved separately.\n",
        "\n",
        "My current implementation of this, however, is not optimal. There is a way of doing this using `torch.stft` and `torchaudio.transforms.Spectrogram` which would allow the transforms to be done on the GPU. However, I can't get it to work well for some reason: the values returned by the STFT are much smaller than those produced by `np.abs(librosa.stft)`. It might have something to do with the way the differing approaches to dealing with the imaginary component of the STFT, which Pytorch tensors cannot handle properly. Then again, it might not.\n",
        "\n",
        "For now, therefore, everything here is using `librosa`. As the dataset is small, the slower speed of the CPU shouldn't be too much of an issue."
      ]
    },
    {
      "metadata": {
        "id": "UAiqOZ2FhwyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AudioFeatureDataset(Dataset):\n",
        "    def __init__(self, dataframe:pd.DataFrame, root_path:Path,\n",
        "                 offset:float=0.0, duration:int=None, sr:int=16000,\n",
        "                 audio_transform=None, image_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: Pandas dataframe of format {labels, filename}\n",
        "            root: root path for input data dir\n",
        "            offset: ?\n",
        "            duration: desired duration of the audio signal\n",
        "            sr: desired sampling rate of the audio signal\n",
        "            transform: transformations to be applied to the data\n",
        "        \"\"\"\n",
        "        self.root = root_path\n",
        "        \n",
        "        self.data = dataframe\n",
        "        self.files = np.array(self.data.iloc[:, 1])\n",
        "        self.classes = np.array(self.data.iloc[:, 0])\n",
        "        \n",
        "        self.sr = sr\n",
        "        self.offset = offset\n",
        "        self.duration = duration\n",
        "        \n",
        "        self.audio_transform = audio_transform\n",
        "        self.image_transform = image_transform\n",
        "        self.librosa = librosa\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Stuff\n",
        "        file = self.files[index]\n",
        "        label = self.classes[index]\n",
        "        \n",
        "        signal, sr = librosa.core.load(self.root / file, sr=self.sr,\n",
        "                                       offset=self.offset,\n",
        "                                       duration=self.duration)\n",
        "        if self.duration:\n",
        "            signal = self._pad_audio(signal)\n",
        "        signal = self.audio_transform(signal)\n",
        "        signal = np.abs(signal)\n",
        "        signal = np.expand_dims(signal, axis=2)\n",
        "        signal = self.image_transform(signal.astype(\"uint8\"))\n",
        "        return signal, label\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def _pad_audio(self, signal):\n",
        "        '''\n",
        "        From https://stackoverflow.com/a/32477869 with changes for clarity\n",
        "        '''\n",
        "        # Calculate target number of samples\n",
        "        n_target = int(self.sr * self.duration)\n",
        "        # Calculate number of zero samples to append\n",
        "        shape = signal.shape\n",
        "        # Create the target shape    \n",
        "        padding = n_target - shape[0]\n",
        "        #   print(\"Padding with %s seconds of silence\" % str(N_pad/fs) )\n",
        "        shape = (padding,) + shape[1:]\n",
        "        # Stack only if there is something to append    \n",
        "        if shape[0] > 0:                \n",
        "            if len(shape) > 1:\n",
        "                return np.vstack((np.zeros(shape),\n",
        "                                signal))\n",
        "            else:\n",
        "                return np.hstack((np.zeros(shape),\n",
        "                                signal))\n",
        "        else:\n",
        "            return signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hk6DMgqxGTUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Short-time Fourier Transform (STFT) features"
      ]
    },
    {
      "metadata": {
        "id": "TwsNs5SEGXkt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Put very briefly, any periodic waveform can be represented by as a sum (possibly infinite) of sinusoids of different frequency and phase. Fourier transforms perform this decomposition on a signal, revealing its frequency and phase components.\n",
        "\n",
        "Fourier transforms are typically applied to the signal as a whole and thus do not reveal how the frequency and phase components of the signal change over time. This can, however, be done with the STFT. First, the signal is divided into overlapping segments of equal length and a window function applied to these segments. The transform is then taken for each of these windows, with the complex-valued results being added to a matrix. \n",
        "\n",
        "The spectrogram of the signal is then defined as the magnitude (the absolute value) of the STFT matrix squared."
      ]
    },
    {
      "metadata": {
        "id": "QPKcy2FXR923",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The STFT of a signal is given by: \n",
        "\n",
        "\\begin{equation*}\n",
        "X[m, \\omega] = \\sum_{k=0}^{\\textrm{win_length}-1}\\textrm{input}[m] \\cdot \\textrm{window}[n - m] \\cdot \\exp{\\bigg(-j\\frac{2\\pi \\cdot \\omega k}{\\textrm{win_length}}\\bigg)}\n",
        "\\end{equation*}"
      ]
    },
    {
      "metadata": {
        "id": "QZKcwPNKR4e1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Varying the window length (`win_length`) affects the resolution of the STFT in terms of time and frequency. Longer windows capture higher frequencies, but show less change across time (as the time domain is split into fewer windows) and shorter windows show greater change in time in exchange for lower frequency resolution. The former are known as narrowband transforms and the latter as wideband transforms."
      ]
    },
    {
      "metadata": {
        "id": "Ia1NauiYPu2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear STFT spectrograms"
      ]
    },
    {
      "metadata": {
        "id": "LEVpeWguQS82",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In linear spectrograms, the STFT frequency bins are scaled linearly which is the unmodified output of the STFT.\n",
        "\n",
        "Both wideband (`win_length` = 512) and narrowband (`win_length` = 2048) linear spectrograms were extracted from the input signal. In both cases the window hop length was $\\textrm{win_len} / 2$. The STFT values were converted to the logarithmic scale (dB) and spectrogram values were normalised to $[-1, 1]$.\n",
        "\n",
        "The final images were resized to 37$\\times$50 pixels for narrowband spectrograms and 154$\\times$12 pixels for wideband spectrograms with Lanczos resampling."
      ]
    },
    {
      "metadata": {
        "id": "pYW8iShuYk6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearSpectrogram(object):\n",
        "    \"\"\"\n",
        "    Creates a spectrogram from a raw audio signal using librosa\n",
        "    This isn't ideal but...\n",
        "    Args:\n",
        "        sr: sample rate\n",
        "        n_fft: size of fft\n",
        "    \"\"\"\n",
        "    def __init__(self, n_fft, sr=16000, hop_length=None, center=True):\n",
        "        self.n_fft = n_fft\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length if hop_length else self.n_fft // 2\n",
        "        self.center = center\n",
        "    \n",
        "    def __call__(self, sig):\n",
        "        stft = np.abs(librosa.core.stft(sig, n_fft=self.n_fft,\n",
        "                                        hop_length=self.hop_length,\n",
        "                                        center=self.center))\n",
        "        spectrogram = librosa.amplitude_to_db(stft, ref=np.max)\n",
        "        return spectrogram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkJ_5d3gaOIQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Narrowband"
      ]
    },
    {
      "metadata": {
        "id": "cUsM0PW2XIpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_linear_params = {\n",
        "    'n_fft': 512\n",
        "}\n",
        "\n",
        "nb_lin_transform = transforms.Compose([\n",
        "    LinearSpectrogram(n_fft=nb_linear_params['n_fft'])\n",
        "])\n",
        "\n",
        "nb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((37, 50), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cbmf_5AZY0jb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below shows an example linear STFT narrowband spectrogram:"
      ]
    },
    {
      "metadata": {
        "id": "euaHIXGlYMPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_linear_stft = AudioFeatureDataset(data_df, ROOT_PATH,\n",
        "                                    audio_transform=nb_lin_transform,\n",
        "                                    image_transform=nb_image_transform,\n",
        "                                    duration=4)\n",
        "image_nblinear = nb_linear_stft.__getitem__(1)[0]\n",
        "\n",
        "plt.imshow(image_nblinear[0], origin='lower')\n",
        "plt.axis('off')\n",
        "plt.title('Narrowband linear spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHM4eA8baTw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wideband"
      ]
    },
    {
      "metadata": {
        "id": "xNGwcYVtaXR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_linear_params = {\n",
        "    'n_fft': 2048\n",
        "}\n",
        "\n",
        "wb_lin_transform = transforms.Compose([\n",
        "    LinearSpectrogram(n_fft=nb_linear_params['n_fft'])\n",
        "])\n",
        "\n",
        "wb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((154, 12), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eqmyZ0h-alTH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wb_linear_stft = AudioFeatureDataset(data_df, ROOT_PATH,\n",
        "                                    audio_transform=wb_lin_transform,\n",
        "                                    image_transform=wb_image_transform,\n",
        "                                    duration=4)\n",
        "image_wblinear = wb_linear_stft.__getitem__(1)[0]\n",
        "\n",
        "plt.imshow(image_wblinear[0], origin='lower')\n",
        "plt.axis('off')\n",
        "plt.title('Wideband linear spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cDNJMdL9bhR5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mel-scale STFT spectrograms"
      ]
    },
    {
      "metadata": {
        "id": "kvKk3_UBbrFR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`librosa.features.melspectrogram()` was used to extract a  [mel scaled](https://en.wikipedia.org/wiki/Mel_scale)  spectrogram by applying a mel filterbank of a specified size to the STFT frequency bin values.\n",
        "\n",
        "128 mel bands were used for narrowband spectrograms and 512 for wideband spectrograms. As with linear spectrograms values were log scaled and normalised to $[-1, 1]$. Images were resized to 37$\\times$50 pixels for narrowband spectrograms and 154$\\times$12 pixels for wideband spectrograms with Lanczos resampling"
      ]
    },
    {
      "metadata": {
        "id": "n0ZZndpAeemH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MelSpectrogram(object):\n",
        "    \"\"\"\n",
        "    Creates a spectrogram from a raw audio signal using librosa\n",
        "    This isn't ideal but...\n",
        "    Args:\n",
        "        sr: sample rate\n",
        "        n_fft: size of fft\n",
        "    \"\"\"\n",
        "    def __init__(self, n_fft, n_mels, sr=16000, hop_length=None):\n",
        "        self.n_fft = n_fft\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length if hop_length else self.n_fft // 2\n",
        "        self.n_mels = n_mels\n",
        "    \n",
        "    def __call__(self, sig):\n",
        "        stft = np.abs(librosa.feature.melspectrogram(sig, n_fft=self.n_fft,\n",
        "                                                     hop_length=self.hop_length,\n",
        "                                                     n_mels=self.n_mels))\n",
        "        spectrogram = librosa.amplitude_to_db(stft, ref=np.max)\n",
        "        return spectrogram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tli0ElP2f3U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Narrowband"
      ]
    },
    {
      "metadata": {
        "id": "WL2ld3NHf5D5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_mel_params = {\n",
        "    'n_fft': 512,\n",
        "    'n_mels': 128\n",
        "}\n",
        "\n",
        "nb_mel_transform = transforms.Compose([\n",
        "    MelSpectrogram(n_fft=nb_mel_params['n_fft'],\n",
        "                  n_mels=nb_mel_params['n_mels'])\n",
        "])\n",
        "\n",
        "nb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((37, 50), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fWsLq3JgNcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_mel_stft = AudioFeatureDataset(data_df, ROOT_PATH,\n",
        "                                    audio_transform=nb_mel_transform,\n",
        "                                    image_transform=nb_image_transform,\n",
        "                                    duration=4)\n",
        "image_nbmel = nb_mel_stft.__getitem__(1)[0]\n",
        "\n",
        "plt.imshow(image_nbmel[0], origin='lower')\n",
        "plt.axis('off')\n",
        "plt.title('Narrowband mel-scaled spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrAupjL_gwlz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wideband"
      ]
    },
    {
      "metadata": {
        "id": "XsitMRWzgy4g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wb_mel_params = {\n",
        "    'n_fft': 2048,\n",
        "    'n_mels': 512\n",
        "}\n",
        "\n",
        "wb_mel_transform = transforms.Compose([\n",
        "    MelSpectrogram(n_fft=wb_mel_params['n_fft'],\n",
        "                  n_mels=wb_mel_params['n_mels'])\n",
        "])\n",
        "\n",
        "wb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((154, 12), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLEUGtYqhEGB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wb_mel_stft = AudioFeatureDataset(data_df, ROOT_PATH,\n",
        "                                    audio_transform=wb_mel_transform,\n",
        "                                    image_transform=wb_image_transform,\n",
        "                                    duration=4)\n",
        "image_wbmel = wb_mel_stft.__getitem__(1)[0]\n",
        "\n",
        "plt.imshow(image_wbmel[0], origin='lower')\n",
        "plt.axis('off')\n",
        "plt.title('Wideband mel-scaled spectrogram')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMiXtS1u8V5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mel Frequency Cepstral Coefficients (MFCCs)"
      ]
    },
    {
      "metadata": {
        "id": "tWRIYAq-k4L4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Along with the four spectral features, mel-frequency cepstral coefficients were also extracted from the raw audio. Per Huzaifah (2017: 2) these were 'obtained using the standard procedure'. That is by:\n",
        "Huzaifah (2017: 2) writes that 'MFCCs were obtained using the standard procedure'. That is by:\n",
        "\n",
        "1. Computing the STFT of the signal\n",
        "2. Applying a mel filterbank to the power spectrum of the signal, producing the mel-scaled STFT\n",
        "3. Taking the logarithm of the powers of each mel frequency\n",
        "4. Taking the Discrete Cosine Transform of the list of log mel powers.\n",
        "\n",
        "The default behaviour of `librosa.feature.mfcc` is to only return the first 20 MFCCs. This was left unchanged as it was assumed to be the approach used by Huzaifah. MFCC values were normalised to $[-1, 1]$ and the image resized to 37$\\times$50 pixels with Lanczos resampling."
      ]
    },
    {
      "metadata": {
        "id": "Z30Jfr-7oimd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MFCC(object):\n",
        "    \"\"\"\n",
        "    Gets MFCCs from a raw audio signal using librosa\n",
        "    Args:\n",
        "        sr: sample rate\n",
        "        n_fft: size of fft\n",
        "    \"\"\"\n",
        "    def __init__(self, sr=16000, n_mfcc=20):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "    \n",
        "    def __call__(self, sig):\n",
        "        mfcc = librosa.feature.mfcc(sig, sr=self.sr, n_mfcc=self.n_mfcc)\n",
        "        return mfcc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TD8XEiHpp2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mfcc_transform = transforms.Compose([\n",
        "    MFCC()\n",
        "])\n",
        "\n",
        "mfcc_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((37, 50), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSEJ5DD3p0It",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mfcc = AudioFeatureDataset(data_df, ROOT_PATH,\n",
        "                                    audio_transform=mfcc_transform,\n",
        "                                    image_transform=mfcc_image_transform,\n",
        "                                    duration=4)\n",
        "image_mfcc = mfcc.__getitem__(15)[0]\n",
        "\n",
        "plt.imshow(image_mfcc[0], origin='lower')\n",
        "plt.axis('off')\n",
        "plt.title('MFCCs')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NNzd8XtaqMQo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN architectures"
      ]
    },
    {
      "metadata": {
        "id": "cUgoX4uIrusZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Huzaifah (2017: 3) defined two CNN architectures, with Conv-5 being deeper than Conv-3. For each network, two different convolutional filters were considered, a  $3\\times3$ filter and an $M\\times3$ filter where $M$ spans the FFT frequency bins. So far, only the $3\\times3$ filter networks are implemented here."
      ]
    },
    {
      "metadata": {
        "id": "kNoCy-i7YGtj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def outputSize(in_size, kernel_size, stride, padding):\n",
        "\n",
        "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
        "    return(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zzl5kXhhuugf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "    \n",
        "class PrintLambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.func(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZ3bc4fsIAhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "hWIMoaU2uv9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Narrowband conv-3"
      ]
    },
    {
      "metadata": {
        "id": "niCt0EMku1tT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_conv3_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 180, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    Lambda(lambda x: x.view(-1, 180 * 9 * 13)),\n",
        "    nn.Linear(180 * 9 * 13, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")\n",
        "\n",
        "# With leaky ReLU\n",
        "nb_conv3__leaky_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 180, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    Lambda(lambda x: x.view(-1, 180 * 9 * 13)),\n",
        "    nn.Linear(180 * 9 * 13, 800),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WyyhBX9ZuzhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wideband conv-3"
      ]
    },
    {
      "metadata": {
        "id": "kBys1pHCIHGn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wb_conv3_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 180, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    Lambda(lambda x: x.view(-1, 180 * 39 * 3)),\n",
        "    nn.Linear(180 * 39 * 3, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")\n",
        "\n",
        "# With leaky ReLU\n",
        "wb_conv3_leaky_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 180, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    Lambda(lambda x: x.view(-1, 180 * 39 * 3)),\n",
        "    nn.Linear(180 * 39 * 3, 800),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYc1sdk9z3DX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "lsPuaACUz5-S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Narrowband conv-5"
      ]
    },
    {
      "metadata": {
        "id": "TJmLKzZRz8nc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_conv5_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Lambda(lambda x: x.view(-1, 96 * 10 * 14)),\n",
        "    nn.Linear(96 * 10 * 14, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")\n",
        "\n",
        "# With leaky ReLU\n",
        "nb_conv5_leaky_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    Lambda(lambda x: x.view(-1, 96 * 10 * 14)),\n",
        "    nn.Linear(96 * 10 * 14, 800),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSa0W_MY58Ba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wideband conv-5"
      ]
    },
    {
      "metadata": {
        "id": "ySUSR2Jo5-rh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wb_conv5_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Lambda(lambda x: x.view(-1, 96 * 40 * 4)),\n",
        "    nn.Linear(96 * 40 * 4, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")\n",
        "\n",
        "# With leaky ReLU\n",
        "wb_conv5_leaky_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
        "    nn.LeakyReLU(),\n",
        "    Lambda(lambda x: x.view(-1, 96 * 40 * 4)),\n",
        "    nn.Linear(96 * 40 * 4, 800),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PX7YInIUoVIR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training CNNs"
      ]
    },
    {
      "metadata": {
        "id": "x8d34CO8oYeB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Class definition"
      ]
    },
    {
      "metadata": {
        "id": "H5vsrm8fofxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defining a class for preparing the data and training the CNNs."
      ]
    },
    {
      "metadata": {
        "id": "EjlQwSnFokyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class _WrappedDataLoader:\n",
        "    def __init__(self, dl, func, dev):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "        self.dev = dev\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b, self.dev))\n",
        "\n",
        "            \n",
        "\n",
        "class FitCNN(object):\n",
        "    \"\"\"\n",
        "    Functions for training a CNN\n",
        "    Args:\n",
        "        train_ds: pytorch Dataset of training data\n",
        "        train_ds: pytorch Dataset of validation data\n",
        "    \"\"\"\n",
        "    def __init__(self, train_ds, valid_ds, bs, preprocess_func, model, epochs,\n",
        "                 loss_func, opt, dev):\n",
        "        self.dev = dev\n",
        "        self.train_ds = train_ds\n",
        "        self.valid_ds = valid_ds\n",
        "        self.bs = bs\n",
        "        self.preprocess = preprocess_func\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.loss_func = loss_func\n",
        "        self.opt = opt\n",
        "        \n",
        "        self.preds = []\n",
        "        self.actuals = []\n",
        "        self.accuracy = []\n",
        "        \n",
        "    \n",
        "    def fit(self):\n",
        "        train_dl, valid_dl = self._get_data(self.train_ds, self.valid_ds, self.bs)\n",
        "        train_dl = _WrappedDataLoader(train_dl, self.preprocess, self.dev)\n",
        "        valid_dl = _WrappedDataLoader(valid_dl, self.preprocess, self.dev)\n",
        "        \n",
        "        self.model.apply(self._weights_init)\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for xb, yb in train_dl:\n",
        "                self._batch_loss(self.model, self.loss_func, xb, yb, self.opt)\n",
        "            \n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                losses, nums = zip(\n",
        "                    *[self._batch_loss(self.model, self.loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "                )\n",
        "                val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "            if epoch % 5 == 0:\n",
        "                print(epoch, val_loss)\n",
        "        self.preds, self.actuals, self.accuracy = self._accuracy_score(valid_dl)\n",
        "    \n",
        "    \n",
        "    def _get_data(self, train_ds, valid_ds, bs):\n",
        "        return (\n",
        "            DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "            DataLoader(valid_ds, batch_size=bs*2)\n",
        "        )\n",
        "    \n",
        "    \n",
        "    def _batch_loss(self, model, loss_func, xb, yb, opt=None):\n",
        "        loss = loss_func(model(xb), yb)\n",
        "\n",
        "        if opt is not None:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        return loss.item(), len(xb)\n",
        "    \n",
        "    \n",
        "    def _accuracy_score(self, valid_dl):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        preds = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for data in valid_dl:\n",
        "                b_images, b_labels = data\n",
        "                outputs = self.model(b_images)\n",
        "                _, b_predicted = torch.max(outputs.data, 1)\n",
        "                total += b_labels.size(0)\n",
        "                correct += (b_predicted == b_labels).sum().item()\n",
        "                labels.extend(b_labels.cpu().tolist())\n",
        "                preds.extend(b_predicted.cpu().cpu().tolist())\n",
        "        accuracy = (100 * correct / total)\n",
        "        print(\"-\" * 40)\n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print(f\"Confusion matrix:\\n {confusion_matrix(labels, preds)}\")\n",
        "        print(\"-\" * 40)\n",
        "        return preds, labels, accuracy\n",
        "        \n",
        "        \n",
        "    def _weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform_(m.weight.data,\n",
        "                                         nn.init.calculate_gain('relu'))\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight.data,\n",
        "                                         nn.init.calculate_gain('relu'))\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XmFtADlnbjF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_nb(x, y, dev):\n",
        "    return x.view(-1, 1, 37, 50).to(dev), y.to(dev)\n",
        "\n",
        "def preprocess_wb(x, y, dev):\n",
        "    return x.view(-1, 1, 154, 12).to(dev), y.to(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1lMR8XVWAXUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross validation"
      ]
    },
    {
      "metadata": {
        "id": "z6s-vOGGDz7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CVModel:\n",
        "    \"\"\"\n",
        "    Create and train a CNN with n-fold cross-validation\n",
        "    \"\"\"\n",
        "    def __init__(self, X, nfolds, root, audio_trfms, image_trfms, dur,\n",
        "                librosa=False, dev=None):\n",
        "        self.X = X\n",
        "        self.y = self.X['label']\n",
        "        self.nfolds = nfolds\n",
        "        \n",
        "        self.root = root\n",
        "        self.audio_trfms = audio_trfms\n",
        "        self.image_trfms = image_trfms\n",
        "        self.dur = dur\n",
        "        self.librosa = librosa\n",
        "        \n",
        "        self.model = ''\n",
        "        self.loss_func = ''\n",
        "        self.opt = ''\n",
        "        self.params = {}\n",
        "        \n",
        "        self.acc_scores = []\n",
        "        self.train_preds = []\n",
        "        self.train_actual = []\n",
        "        \n",
        "        if dev:\n",
        "            self.dev = dev\n",
        "        elif torch.cuda.is_available():\n",
        "            self.dev = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.dev = torch.device(\"cpu\")\n",
        "    \n",
        "    def train_cnn(self, model, params):\n",
        "        # CV setup\n",
        "#         dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        self.model = model.to(self.dev)\n",
        "        self.params = params\n",
        "        \n",
        "        skf = StratifiedKFold(n_splits=self.nfolds, random_state=42, shuffle=True)\n",
        "        fold_count = 1\n",
        "        \n",
        "        for train_index, valid_index in skf.split(np.zeros(self.X.shape[0]), self.y):\n",
        "            print(f\"Fold {fold_count} / {self.nfolds}:\")\n",
        "            # Splits data into training and validation sets\n",
        "            X_train, X_valid = self.X.iloc[train_index], self.X.iloc[valid_index]\n",
        "            \n",
        "            train_ds = AudioFeatureDataset(X_train, self.root,\n",
        "                                           audio_transform=self.audio_trfms,\n",
        "                                           image_transform=self.image_trfms,\n",
        "                                           duration=self.dur,\n",
        "                                           librosa=self.librosa)\n",
        "                        \n",
        "            valid_ds = AudioFeatureDataset(X_valid, self.root,\n",
        "                                           audio_transform=self.audio_trfms,\n",
        "                                           image_transform=self.image_trfms,\n",
        "                                           duration=self.dur,\n",
        "                                           librosa=self.librosa)\n",
        "            \n",
        "            # Trains the model and gets predictions and accuracy scores\n",
        "            opt = optim.Adam(self.model.parameters(),\n",
        "                             lr=self.params['lr'],\n",
        "                             weight_decay=self.params['l2'])\n",
        "            loss_func = nn.CrossEntropyLoss()\n",
        "            \n",
        "            model_fitter = FitCNN(train_ds, valid_ds, bs=self.params['bs'],\n",
        "                                  preprocess_func=self.params['preprocess'],\n",
        "                                  model=self.model,\n",
        "                                  epochs=self.params['epochs'],\n",
        "                                  loss_func=loss_func,\n",
        "                                  opt=opt, dev=self.dev)\n",
        "            \n",
        "            \n",
        "            model_fitter.fit()\n",
        "\n",
        "            # Process model results\n",
        "            self.train_preds.extend(model_fitter.preds)\n",
        "            self.train_actual.extend(model_fitter.actuals)\n",
        "            self.acc_scores.append(model_fitter.accuracy)\n",
        "            fold_count += 1\n",
        "            \n",
        "        # Print model results summary\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Accuracy scores: {self.acc_scores}\")\n",
        "        print(f\"Mean accuracy: {np.mean(self.acc_scores)}\")\n",
        "        print(f\"Accuracy sd: {np.std(self.acc_scores)}\")\n",
        "        print(f\"Confusion matrix:\\n{confusion_matrix(self.train_actual, self.train_preds)}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Model parameters:\\n {self.params}\")\n",
        "        print(\"=\" * 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJ1R7gpuk65N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Narrowband linear STFT: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "wmZf9hBJJyDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nblinear_conv3_params = {\n",
        "    'bs': 40,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 50,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nblinear_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_lin_transform, nb_image_transform,\n",
        "                        4)\n",
        "nblinear_conv3.train_cnn(nb_conv3_model,\n",
        "                        nblinear_conv3_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9R2Qe25q2SbY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Narrowband linear STFT: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "WJBjWp9F2Wkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nblinear_conv5_params = {\n",
        "    'bs': 40,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nblinear_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_lin_transform, nb_image_transform,\n",
        "                        4)\n",
        "nblinear_conv5.train_cnn(nb_conv5_model,\n",
        "                        nblinear_conv5_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqdrLdbGt4b6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wideband linear STFT: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "XUMPCnODt8V7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wblinear_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wblinear_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_lin_transform, wb_image_transform,\n",
        "                        4)\n",
        "wblinear_conv3.train_cnn(wb_conv3_model,\n",
        "                        wblinear_conv3_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MpuHd32G5uG9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wideband Linear STFT: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "hRSxlYAt5zOS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wblinear_conv5_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wblinear_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_lin_transform, wb_image_transform,\n",
        "                        4)\n",
        "wblinear_conv5.train_cnn(wb_conv5_model,\n",
        "                        wblinear_conv5_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmdjmZ56ecd8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Narrowband mel-scale STFT: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "HvlP1_xKrzzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nbmel_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 100,\n",
        "    'lr': 0.005,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nbmel_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_mel_transform, nb_image_transform,\n",
        "                        4)\n",
        "nbmel_conv3.train_cnn(nb_conv3_model,\n",
        "                        nbmel_conv3_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RyvLEdRC8gNN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Narrowband mel-scale STFT: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "2nyLB7-28j51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nbmel_conv5_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nbmel_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_mel_transform, nb_image_transform,\n",
        "                        4)\n",
        "nbmel_conv5.train_cnn(nb_conv5_model,\n",
        "                        nbmel_conv5_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6oKRcD03aKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wideband mel-scale STFT: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "nf6ZR1n03fuA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wbmel_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wbmel_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_mel_transform, wb_image_transform,\n",
        "                        4)\n",
        "wbmel_conv3.train_cnn(wb_conv3_model,\n",
        "                        wbmel_conv3_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZ9Rhat3BgPT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WIdeband mel-scale STFT: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "21QLmASyBjVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wbmel_conv5_params = {\n",
        "    'bs': 40,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wbmel_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_mel_transform, wb_image_transform,\n",
        "                        4)\n",
        "# wbmel_conv5.train_cnn(wb_conv5_model,\n",
        "#                         wbmel_conv5_params)\n",
        "\n",
        "wbmel_conv5.train_cnn(wb_conv5_leaky_model,\n",
        "                        wbmel_conv5_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxuiJj457q13",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MFCCs: Conv-3"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ikU94DB1NqzU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "mfcc_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "mfcc_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        mfcc_transform, nb_image_transform,\n",
        "                        4)\n",
        "mfcc_conv3.train_cnn(nb_conv3_model,\n",
        "                        mfcc_conv3_params)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}