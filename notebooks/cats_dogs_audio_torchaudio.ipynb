{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats_dogs_audio_torchaudio.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thirdformant/cats_dogs_audio/blob/master/notebooks/cats_dogs_audio_torchaudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yNa0zvIFtgZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Audio classification of cats and dogs"
      ]
    },
    {
      "metadata": {
        "id": "hRdNgGMetuHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training a machine learning classification model on images of cats and dogs is a common introductory problem. With modern convolutional neural network (CNN) architectures and transfer learning, it is now possible to achieve near-perfect levels of classification accuracy as demonstrated very effectively (and efficiently!) early on in the [fast.ai deep learning MOOC](https://course.fast.ai/). When browsing through the datasets on Kaggle some time ago, I stumbled upon the [Audio Cats and Dogs](https://www.kaggle.com/mmoreaux/audio-cats-and-dogs/home) data, which presents itself as 'the audio counterpart' to the typical image classification problem. As someone with a background in acoustic phonetics, this has rather a lot of appeal. In addition, the dataset brings some new challenges to classification:  first, the audio data requires considerable preprocessing if a CNN is to be used, and second, the data consists of only 277 files. More on both these topics later.\n",
        "\n",
        "This notebook comprises two major parts. The first is a partial implementation of the approaches described in Huzaifah (2017) in his experimentation regarding CNN-based classification of environmental sound data. While the CNN architectures he used were implemented in TensorFlow, I have chosen instead to use Pytorch. The second section ((improves on the approaches used by Huzaifah, increasing the overall classification accuracy using TODO: finish this once the implementation is done))."
      ]
    },
    {
      "metadata": {
        "id": "Pjyov0JX10vr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries and setup"
      ]
    },
    {
      "metadata": {
        "id": "-tJQoLFbc2ld",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Librosa](http://librosa.github.io/librosa/) is a general-purpose audio processing and analysis library."
      ]
    },
    {
      "metadata": {
        "id": "oCMQApJuiRft",
        "colab_type": "code",
        "outputId": "4c572f64-13fe-48c2-db0d-21b5827549cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2197
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install sox libsox-dev libsox-fmt-all\n",
        "!pip3 install git+git://github.com/pytorch/audio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libmagic-mgc libmagic1\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-ao\n",
            "  libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse libsox3\n",
            "Suggested packages:\n",
            "  libaudio2 file\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libmagic-mgc libmagic1\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-dev libsox-fmt-all\n",
            "  libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss\n",
            "  libsox-fmt-pulse libsox3 sox\n",
            "0 upgraded, 18 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 1,267 kB of archives.\n",
            "After this operation, 9,144 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.1 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.1 [68.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libao-common all 1.2.2+20180113-1ubuntu1 [6,644 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libao4 amd64 1.2.2+20180113-1ubuntu1 [35.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox3 amd64 14.4.2-3 [225 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-alsa amd64 14.4.2-3 [10.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-ao amd64 14.4.2-3 [7,452 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-base amd64 14.4.2-3 [32.0 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3 [15.8 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-oss amd64 14.4.2-3 [9,004 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-pulse amd64 14.4.2-3 [7,372 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-fmt-all amd64 14.4.2-3 [5,116 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsox-dev amd64 14.4.2-3 [325 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 sox amd64 14.4.2-3 [101 kB]\n",
            "Fetched 1,267 kB in 1s (1,137 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 131352 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../02-libmagic-mgc_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../03-libmagic1_1%3a5.32-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libao-common.\n",
            "Preparing to unpack .../04-libao-common_1.2.2+20180113-1ubuntu1_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1ubuntu1) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../05-libao4_1.2.2+20180113-1ubuntu1_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1ubuntu1) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../06-libid3tag0_0.15.1b-13_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../07-libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../08-libsox3_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../09-libsox-fmt-alsa_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-ao_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-base_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-mp3_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-oss_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-pulse_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../15-libsox-fmt-all_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../16-libsox-dev_14.4.2-3_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2-3) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../17-sox_14.4.2-3_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1ubuntu1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.1) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2-3) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1ubuntu1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2-3) ...\n",
            "Setting up sox (14.4.2-3) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2-3) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2-3) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting git+git://github.com/pytorch/audio\n",
            "  Cloning git://github.com/pytorch/audio to /tmp/pip-req-build-eqs_vjv8\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchaudio==0.2) (1.0.1.post2)\n",
            "Building wheels for collected packages: torchaudio\n",
            "  Building wheel for torchaudio (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-l29t104v/wheels/4e/7c/c5/0d946acbaccad9fe62590374454c4cf135846c9c96fce3ac75\n",
            "Successfully built torchaudio\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lDu3Ygur2Mu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import librosa # Audio library\n",
        "\n",
        "# Data viz\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torchaudio.transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6PjxjQJBwFn",
        "colab_type": "code",
        "outputId": "bd03f7ae-f4b7-45ad-9c1d-15a01f0feff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "vtQnctniBqGB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_cuda = True\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXP80igW8tkj",
        "colab_type": "code",
        "outputId": "c35013de-43cc-4db0-d775-29e8ef2e457f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "# Connecting to google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eqsk_vjC2oGJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reading data"
      ]
    },
    {
      "metadata": {
        "id": "vzTyjOv58Zs6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I created a csv file containing the filename and the file labels. This can be passed to the Pytorch Dataset class defined in the next section."
      ]
    },
    {
      "metadata": {
        "id": "PpkduA0421es",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "ROOT_PATH = Path('/content/gdrive/My Drive/data/cats_dogs_audio/cats_dogs/')\n",
        "CSV_PATH = Path('/content/gdrive/My Drive/data/cats_dogs_audio/all_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3vzPD50qjsy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Creates the csv file. Commented out because I only needed to run it once\n",
        "\n",
        "# all_data = {\n",
        "#     'label': [],\n",
        "#     'filename': []\n",
        "# }\n",
        "\n",
        "\n",
        "# files_list = os.listdir(ROOT_PATH)\n",
        "# for f in files_list:\n",
        "#     if Path(f).suffix == '.wav':\n",
        "#         all_data['filename'].append(f)\n",
        "#         if 'cat' in f:\n",
        "#             all_data['label'].append(0)\n",
        "#         else:\n",
        "#             all_data['label'].append(1)\n",
        "\n",
        "# data_df = pd.DataFrame(all_data).iloc[:, ::-1] # Reverse the column order\n",
        "# data_df.to_csv(CSV_PATH, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLhW72lJ-PO3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(CSV_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rVGFQB_AwArl",
        "colab_type": "code",
        "outputId": "d92d5ce2-fdfc-4bc0-c88e-0fd5ee8d9b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(f'The distribution of classes in the data is: {Counter(data_df[\"label\"])}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The distribution of classes in the data is: Counter({0: 164, 1: 113})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NNzd8XtaqMQo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN architectures"
      ]
    },
    {
      "metadata": {
        "id": "cUgoX4uIrusZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Huzaifah (2017: 3) defined two CNN architectures, with Conv-5 being deeper than Conv-3. For each network, two different convolutional filters were considered, a  $3\\times3$ filter and an $M\\times3$ filter where $M$ spans the FFT frequency bins. So far, only the $3\\times3$ filter networks are implemented here."
      ]
    },
    {
      "metadata": {
        "id": "kNoCy-i7YGtj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def outputSize(in_size, kernel_size, stride, padding):\n",
        "\n",
        "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
        "    return(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zzl5kXhhuugf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "    \n",
        "class PrintLambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.func(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZ3bc4fsIAhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "hWIMoaU2uv9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Narrowband conv-3"
      ]
    },
    {
      "metadata": {
        "id": "niCt0EMku1tT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_conv3_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 180, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    Lambda(lambda x: x.view(-1, 180 * 9 * 13)),\n",
        "    nn.Linear(180 * 9 * 13, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WyyhBX9ZuzhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wideband conv-3"
      ]
    },
    {
      "metadata": {
        "id": "kBys1pHCIHGn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wb_conv3_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 180, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=4, stride=4, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    Lambda(lambda x: x.view(-1, 180 * 39 * 3)),\n",
        "    nn.Linear(180 * 39 * 3, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYc1sdk9z3DX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "lsPuaACUz5-S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Narrowband conv-5"
      ]
    },
    {
      "metadata": {
        "id": "TJmLKzZRz8nc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_conv5_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Lambda(lambda x: x.view(-1, 96 * 10 * 14)),\n",
        "    nn.Linear(96 * 10 * 14, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSa0W_MY58Ba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Wideband conv-5"
      ]
    },
    {
      "metadata": {
        "id": "ySUSR2Jo5-rh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wb_conv5_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 24, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "    nn.Conv2d(48, 96, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    Lambda(lambda x: x.view(-1, 96 * 40 * 4)),\n",
        "    nn.Linear(96 * 40 * 4, 800),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(800, 2)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFmGMBbRefOX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN training functions"
      ]
    },
    {
      "metadata": {
        "id": "H5vsrm8fofxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Defining a class for preparing the data and training the CNNs."
      ]
    },
    {
      "metadata": {
        "id": "EjlQwSnFokyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class _WrappedDataLoader:\n",
        "    def __init__(self, dl, func, dev):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "        self.dev = dev\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b, self.dev))\n",
        "\n",
        "            \n",
        "\n",
        "class FitCNN(object):\n",
        "    \"\"\"\n",
        "    Functions for training a CNN\n",
        "    Args:\n",
        "        train_ds: pytorch Dataset of training data\n",
        "        train_ds: pytorch Dataset of validation data\n",
        "    \"\"\"\n",
        "    def __init__(self, train_ds, valid_ds, bs, preprocess_func, model, epochs,\n",
        "                 loss_func, opt, dev):\n",
        "        self.dev = dev\n",
        "        self.train_ds = train_ds\n",
        "        self.valid_ds = valid_ds\n",
        "        self.bs = bs\n",
        "        self.preprocess = preprocess_func\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.loss_func = loss_func\n",
        "        self.opt = opt\n",
        "        \n",
        "        self.preds = []\n",
        "        self.actuals = []\n",
        "        self.accuracy = []\n",
        "        \n",
        "    \n",
        "    def fit(self):\n",
        "        train_dl, valid_dl = self._get_data(self.train_ds, self.valid_ds, self.bs)\n",
        "        train_dl = _WrappedDataLoader(train_dl, self.preprocess, self.dev)\n",
        "        valid_dl = _WrappedDataLoader(valid_dl, self.preprocess, self.dev)\n",
        "        \n",
        "        if self.dev == torch.device(\"cuda\"):\n",
        "            print(\"Training the model on the GPU\")\n",
        "        else:\n",
        "            print(\"Training the model on the CPU\")\n",
        "        \n",
        "        self.model.apply(self._weights_init)\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for xb, yb in train_dl:\n",
        "                self._batch_loss(self.model, self.loss_func, xb, yb, self.opt)\n",
        "            \n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                losses, nums = zip(\n",
        "                    *[self._batch_loss(self.model, self.loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "                )\n",
        "                val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "            if epoch % 5 == 0:\n",
        "                print(epoch, val_loss)\n",
        "        self.preds, self.actuals, self.accuracy = self._accuracy_score(valid_dl)\n",
        "    \n",
        "    \n",
        "    def _get_data(self, train_ds, valid_ds, bs):\n",
        "        return (\n",
        "            DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "            DataLoader(valid_ds, batch_size=bs*2)\n",
        "        )\n",
        "    \n",
        "    \n",
        "    def _batch_loss(self, model, loss_func, xb, yb, opt=None):\n",
        "        loss = loss_func(model(xb), yb)\n",
        "\n",
        "        if opt is not None:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        return loss.item(), len(xb)\n",
        "    \n",
        "    \n",
        "    def _accuracy_score(self, valid_dl):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        preds = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for data in valid_dl:\n",
        "                b_images, b_labels = data\n",
        "                outputs = self.model(b_images)\n",
        "                _, b_predicted = torch.max(outputs.data, 1)\n",
        "                total += b_labels.size(0)\n",
        "                correct += (b_predicted == b_labels).sum().item()\n",
        "                labels.extend(b_labels.cpu().tolist())\n",
        "                preds.extend(b_predicted.cpu().cpu().tolist())\n",
        "        accuracy = (100 * correct / total)\n",
        "        print(\"-\" * 40)\n",
        "        print(f'Accuracy: {accuracy}')\n",
        "        print(f\"Confusion matrix:\\n {confusion_matrix(labels, preds)}\")\n",
        "        print(\"-\" * 40)\n",
        "        return preds, labels, accuracy\n",
        "        \n",
        "        \n",
        "    def _weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform_(m.weight.data,\n",
        "                                         nn.init.calculate_gain('relu'))\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight.data,\n",
        "                                         nn.init.calculate_gain('relu'))\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ka2p1u6Bexdo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Cross validation"
      ]
    },
    {
      "metadata": {
        "id": "62KfS-Zte1Rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Class for n-fold cross-validation of CNN."
      ]
    },
    {
      "metadata": {
        "id": "YfdzmO2MewCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CVModel:\n",
        "    \"\"\"\n",
        "    Create and train a CNN with n-fold cross-validation\n",
        "    \"\"\"\n",
        "    def __init__(self, X, nfolds, root, audio_trfms, image_trfms, dur,\n",
        "                librosa=False, dev=None):\n",
        "        self.X = X\n",
        "        self.y = self.X['label']\n",
        "        self.nfolds = nfolds\n",
        "        \n",
        "        self.root = root\n",
        "        self.audio_trfms = audio_trfms\n",
        "        self.image_trfms = image_trfms\n",
        "        self.dur = dur\n",
        "        self.librosa = librosa\n",
        "        \n",
        "        self.model = ''\n",
        "        self.loss_func = ''\n",
        "        self.opt = ''\n",
        "        self.params = {}\n",
        "        \n",
        "        self.acc_scores = []\n",
        "        self.train_preds = []\n",
        "        self.train_actual = []\n",
        "        \n",
        "        if dev:\n",
        "            self.dev = dev\n",
        "        elif torch.cuda.is_available():\n",
        "            self.dev = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.dev = torch.device(\"cpu\")\n",
        "    \n",
        "    def train_cnn(self, model, params):\n",
        "        # CV setup\n",
        "        self.model = model.to(self.dev)\n",
        "        self.params = params\n",
        "        \n",
        "        skf = StratifiedKFold(n_splits=self.nfolds, random_state=42, shuffle=True)\n",
        "        fold_count = 1\n",
        "        \n",
        "        for train_index, valid_index in skf.split(np.zeros(self.X.shape[0]), self.y):\n",
        "            print(f\"Fold {fold_count} / {self.nfolds}:\")\n",
        "            # Splits data into training and validation sets\n",
        "            X_train, X_valid = self.X.iloc[train_index], self.X.iloc[valid_index]\n",
        "            \n",
        "            train_ds = AudioFeatureDataset(X_train, self.root,\n",
        "                                           audio_transform=self.audio_trfms,\n",
        "                                           image_transform=self.image_trfms,\n",
        "                                           duration=self.dur,\n",
        "                                           librosa=self.librosa)\n",
        "                        \n",
        "            valid_ds = AudioFeatureDataset(X_valid, self.root,\n",
        "                                           audio_transform=self.audio_trfms,\n",
        "                                           image_transform=self.image_trfms,\n",
        "                                           duration=self.dur,\n",
        "                                           librosa=self.librosa)\n",
        "            \n",
        "            # Trains the model and gets predictions and accuracy scores\n",
        "            opt = optim.Adam(self.model.parameters(),\n",
        "                             lr=self.params['lr'],\n",
        "                             weight_decay=self.params['l2'])\n",
        "            loss_func = nn.CrossEntropyLoss()\n",
        "            \n",
        "            model_fitter = FitCNN(train_ds, valid_ds, bs=self.params['bs'],\n",
        "                                  preprocess_func=self.params['preprocess'],\n",
        "                                  model=self.model,\n",
        "                                  epochs=self.params['epochs'],\n",
        "                                  loss_func=loss_func,\n",
        "                                  opt=opt, dev=self.dev)\n",
        "            \n",
        "            \n",
        "            model_fitter.fit()\n",
        "\n",
        "            # Process model results\n",
        "            self.train_preds.extend(model_fitter.preds)\n",
        "            self.train_actual.extend(model_fitter.actuals)\n",
        "            self.acc_scores.append(model_fitter.accuracy)\n",
        "            fold_count += 1\n",
        "            \n",
        "        # Print model results summary\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"Accuracy scores: {self.acc_scores}\")\n",
        "        print(f\"Mean accuracy: {np.mean(self.acc_scores)}\")\n",
        "        print(f\"Accuracy sd: {np.std(self.acc_scores)}\")\n",
        "        print(f\"Median accuracy: {np.median(self.acc_scores)}\")\n",
        "        print(f\"Confusion matrix:\\n{confusion_matrix(self.train_actual, self.train_preds)}\")\n",
        "        print(\"-\" * 40)\n",
        "        print(f\"Model parameters:\\n {self.params}\")\n",
        "        print(\"=\" * 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRtIXaN9hKFL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_nb(x, y, dev):\n",
        "    return x.view(-1, 1, 37, 50).to(dev), y.to(dev)\n",
        "\n",
        "def preprocess_wb(x, y, dev):\n",
        "    return x.view(-1, 1, 154, 12).to(dev), y.to(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FMnU7-ElJ3Jt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN training"
      ]
    },
    {
      "metadata": {
        "id": "nQ0DbNpCfHPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature overview"
      ]
    },
    {
      "metadata": {
        "id": "dDBHyFN3-TTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Waveforms show changes in the signal, especially in its amplitude (loudness), over time and are therefore representations of the signal in the *time domain*. This is, however, only one aspect of the signal. When it comes to training audio classification or e.g. speech recognition models, better results are achieved when considering how the frequency of the signal, rather than its amplitude, changes over time. These are known as *time-frequency* representations, the best-known of which is the spectrogram.\n",
        "\n",
        "While Huzaifah (2017) created four time-frequency representations, only two are currently implemented here:\n",
        "- Linear-scaled Short-time Fourier Transform (STFT) spectrograms\n",
        "- mel-scaled STFT spectrograms.\n",
        "\n",
        "In addition, Mel-frequency Cepstral Coefficients (MFCCs) were also extracted from the input signal.\n",
        "\n",
        "Prior to feature extractions, all raw audio was either clipped or padded to a 4 second duration."
      ]
    },
    {
      "metadata": {
        "id": "tHz754Fy--eo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pytorch dataset definition"
      ]
    },
    {
      "metadata": {
        "id": "VeB86ajfYQXS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pytorch datasets allow the audio features to be extracted and transformed 'on the fly' when needed, rather than being extracted in advance and saved separately.\n",
        "\n",
        "The current implementation of this allows for two options. The first uses `librosa` to load the audio data and perform the transformations. This was the approach taken by Huzaifah (2017) and so produces the most directly comparable results. However, `librosa` is limited to the CPU and is therefore slow. The second option uses the transformations in `torchaudio`, which can be run on a CUDA-compatible GPU. Being the faster option, this is the default here."
      ]
    },
    {
      "metadata": {
        "id": "UAiqOZ2FhwyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AudioFeatureDataset(Dataset):\n",
        "    def __init__(self, dataframe:pd.DataFrame, root_path:Path,\n",
        "                 offset:int=0, duration:int=None, sr:int=16000,\n",
        "                 audio_transform=None, image_transform=None,\n",
        "                 librosa=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: Pandas dataframe of format {labels, filename}\n",
        "            root: root path for input data dir\n",
        "            offset: ?\n",
        "            duration: desired duration of the audio signal\n",
        "            sr: desired sampling rate of the audio signal\n",
        "            transform: transformations to be applied to the data\n",
        "        \"\"\"\n",
        "        self.root = root_path\n",
        "        \n",
        "        self.data = dataframe\n",
        "        self.files = np.array(self.data.iloc[:, 1])\n",
        "        self.classes = np.array(self.data.iloc[:, 0])\n",
        "        \n",
        "        self.sr = sr\n",
        "        self.offset = offset\n",
        "        self.duration = duration\n",
        "        \n",
        "        self.audio_transform = audio_transform\n",
        "        self.image_transform = image_transform\n",
        "        self.librosa = librosa\n",
        "        \n",
        "        self.dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # Stuff\n",
        "        file = self.files[index]\n",
        "        label = self.classes[index]\n",
        "        \n",
        "        if self.librosa:\n",
        "            signal, sr = librosa.core.load(self.root / file, sr=self.sr,\n",
        "                                       offset=self.offset,\n",
        "                                       duration=self.duration)\n",
        "            if self.duration:\n",
        "                signal = self._pad_audio(signal)\n",
        "                \n",
        "            signal = self.audio_transform(signal)\n",
        "            signal = np.abs(signal)\n",
        "            signal = np.expand_dims(signal, axis=2)\n",
        "            signal = self.image_transform(signal.astype(\"uint8\"))\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            signal = torchaudio.load(self.root / file, offset=self.offset)[0].to(self.dev)\n",
        "        \n",
        "            if self.duration:\n",
        "                num_samples = self.sr * self.duration\n",
        "                pad_trfms = transforms.Compose([\n",
        "                    torchaudio.transforms.PadTrim(max_len=num_samples)])\n",
        "                signal = pad_trfms(signal)\n",
        "            \n",
        "            signal = self.audio_transform(signal)\n",
        "            signal = signal.transpose(1, 2)\n",
        "            signal = self.image_transform(signal.cpu().type(torch.uint8))\n",
        "\n",
        "        return signal, label\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def _pad_audio(self, signal):\n",
        "        '''\n",
        "        From https://stackoverflow.com/a/32477869 with comments and minor\n",
        "        changes to variable names for additional clarity\n",
        "        '''\n",
        "        # Calculate target number of samples\n",
        "        n_target = int(self.sr * self.duration)\n",
        "        # Calculate number of zero samples to append\n",
        "        shape = signal.shape\n",
        "        # Create the target shape    \n",
        "        padding = n_target - shape[0]\n",
        "        #   print(\"Padding with %s seconds of silence\" % str(N_pad/fs) )\n",
        "        shape = (padding,) + shape[1:]\n",
        "        # Stack only if there is something to append    \n",
        "        if shape[0] > 0:                \n",
        "            if len(shape) > 1:\n",
        "                return np.vstack((np.zeros(shape),\n",
        "                                signal))\n",
        "            else:\n",
        "                return np.hstack((np.zeros(shape),\n",
        "                                signal))\n",
        "        else:\n",
        "            return signal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hk6DMgqxGTUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Short-time Fourier Transform (STFT) features"
      ]
    },
    {
      "metadata": {
        "id": "TwsNs5SEGXkt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Put very briefly, any periodic waveform can be represented by as a sum (possibly infinite) of sinusoids of different frequency and phase. Fourier transforms perform this decomposition on a signal, revealing its frequency and phase components.\n",
        "\n",
        "Fourier transforms are typically applied to the signal as a whole and thus do not reveal how the frequency and phase components of the signal change over time. This can, however, be done with the STFT. First, the signal is divided into overlapping segments of equal length and a window function applied to these segments. The transform is then taken for each of these windows, with the complex-valued results being added to a matrix. \n",
        "\n",
        "The spectrogram of the signal is then defined as the magnitude (the absolute value) of the STFT matrix squared."
      ]
    },
    {
      "metadata": {
        "id": "QPKcy2FXR923",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The STFT of a signal is given by: \n",
        "\n",
        "\\begin{equation*}\n",
        "X[m, \\omega] = \\sum_{k=0}^{\\textrm{win_length}-1}\\textrm{input}[m] \\cdot \\textrm{window}[n - m] \\cdot \\exp{\\bigg(-j\\frac{2\\pi \\cdot \\omega k}{\\textrm{win_length}}\\bigg)}\n",
        "\\end{equation*}"
      ]
    },
    {
      "metadata": {
        "id": "QZKcwPNKR4e1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Varying the window length (`win_length`) affects the resolution of the STFT in terms of time and frequency. Longer windows capture higher frequencies, but show less change across time (as the time domain is split into fewer windows) and shorter windows show greater change in time in exchange for lower frequency resolution. The former are known as narrowband transforms and the latter as wideband transforms."
      ]
    },
    {
      "metadata": {
        "id": "Ia1NauiYPu2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear STFT spectrograms"
      ]
    },
    {
      "metadata": {
        "id": "LEVpeWguQS82",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In linear spectrograms, the STFT frequency bins are scaled linearly which is the unmodified output of the STFT.\n",
        "\n",
        "Both wideband (`win_length` = 512) and narrowband (`win_length` = 2048) linear spectrograms were extracted from the input signal. In both cases the window hop length was $\\textrm{win_len} / 2$. The STFT values were converted to the logarithmic scale (dB) and spectrogram values were normalised to $[-1, 1]$.\n",
        "\n",
        "The final images were resized to 37$\\times$50 pixels for narrowband spectrograms and 154$\\times$12 pixels for wideband spectrograms with Lanczos resampling."
      ]
    },
    {
      "metadata": {
        "id": "pYW8iShuYk6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearSpectrogram(object):\n",
        "    \"\"\"\n",
        "    Creates a spectrogram from a raw audio signal using librosa\n",
        "    Args:\n",
        "        sr: sample rate\n",
        "        n_fft: size of fft\n",
        "    \"\"\"\n",
        "    def __init__(self, n_fft, sr=16000, hop_length=None, center=True):\n",
        "        self.n_fft = n_fft\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length if hop_length else self.n_fft // 2\n",
        "        self.center = center\n",
        "    \n",
        "    def __call__(self, sig):\n",
        "        stft = np.abs(librosa.core.stft(sig, n_fft=self.n_fft,\n",
        "                                        hop_length=self.hop_length,\n",
        "                                        center=self.center))\n",
        "        spectrogram = librosa.amplitude_to_db(stft, ref=np.max)\n",
        "        return spectrogram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cUsM0PW2XIpP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Parameters and transformations for narrowband linear STFT features\n",
        "\"\"\"\n",
        "nb_linear_params = {\n",
        "    'n_fft': 512\n",
        "}\n",
        "\n",
        "nb_lin_transform_librosa = transforms.Compose([\n",
        "    LinearSpectrogram(n_fft=nb_linear_params['n_fft'])\n",
        "])\n",
        "\n",
        "nb_lin_transform = transforms.Compose([\n",
        "    torchaudio.transforms.Spectrogram(ws=nb_linear_params['n_fft']),\n",
        "    torchaudio.transforms.SpectogramToDB()\n",
        "])\n",
        "\n",
        "nb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((37, 50), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNGwcYVtaXR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Parameters and transformations for wideband linear STFT features\n",
        "\"\"\"\n",
        "\n",
        "wb_linear_params = {\n",
        "    'n_fft': 2048\n",
        "}\n",
        "\n",
        "wb_lin_transform_librosa = transforms.Compose([\n",
        "    LinearSpectrogram(n_fft=wb_linear_params['n_fft'])\n",
        "])\n",
        "\n",
        "wb_lin_transform = transforms.Compose([\n",
        "    torchaudio.transforms.Spectrogram(ws=wb_linear_params['n_fft']),\n",
        "    torchaudio.transforms.SpectogramToDB()\n",
        "])\n",
        "\n",
        "wb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((154, 12), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GkJ_5d3gaOIQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Narrowband: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "KnIpBlfMfqIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1875
        },
        "outputId": "1916e565-f191-48ab-ed8f-1d5d02c885d7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nblinear_conv3_params = {\n",
        "    'bs': 40,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 50,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nblinear_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_lin_transform, nb_image_transform,\n",
        "                        4)\n",
        "nblinear_conv3.train_cnn(nb_conv3_model,\n",
        "                        nblinear_conv3_params)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 2.092751979827881\n",
            "5 1.1619735956192017\n",
            "10 0.5669213533401489\n",
            "15 0.5298570990562439\n",
            "20 0.49531981348991394\n",
            "25 0.5233623385429382\n",
            "30 0.4440250098705292\n",
            "35 0.4984112083911896\n",
            "40 0.4452494978904724\n",
            "45 0.43203622102737427\n",
            "----------------------------------------\n",
            "Accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            " [[29  4]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 5.152708530426025\n",
            "5 0.5642483830451965\n",
            "10 0.3404325544834137\n",
            "15 0.3879304826259613\n",
            "20 0.26762905716896057\n",
            "25 0.2525107264518738\n",
            "30 0.243948832154274\n",
            "35 0.29726743698120117\n",
            "40 0.2576000988483429\n",
            "45 0.277050644159317\n",
            "----------------------------------------\n",
            "Accuracy: 94.64285714285714\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 2 21]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 1.0053571462631226\n",
            "5 2.9141790866851807\n",
            "10 0.646515429019928\n",
            "15 0.4534628093242645\n",
            "20 0.4643685221672058\n",
            "25 0.48675909638404846\n",
            "30 0.4805963933467865\n",
            "35 0.4726196825504303\n",
            "40 0.497299462556839\n",
            "45 0.3954138457775116\n",
            "----------------------------------------\n",
            "Accuracy: 78.57142857142857\n",
            "Confusion matrix:\n",
            " [[23 10]\n",
            " [ 2 21]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 3.7492339611053467\n",
            "5 0.40183115005493164\n",
            "10 0.3705386221408844\n",
            "15 0.31573769450187683\n",
            "20 0.34470903873443604\n",
            "25 0.27535775303840637\n",
            "30 0.3651108741760254\n",
            "35 0.33318448066711426\n",
            "40 0.294003427028656\n",
            "45 0.3045327961444855\n",
            "----------------------------------------\n",
            "Accuracy: 92.72727272727273\n",
            "Confusion matrix:\n",
            " [[33  0]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 2.204310417175293\n",
            "5 0.8057101964950562\n",
            "10 0.5052075386047363\n",
            "15 0.42977097630500793\n",
            "20 0.39069297909736633\n",
            "25 0.36726951599121094\n",
            "30 0.3764134645462036\n",
            "35 0.3637978732585907\n",
            "40 0.36102351546287537\n",
            "45 0.37047049403190613\n",
            "----------------------------------------\n",
            "Accuracy: 81.48148148148148\n",
            "Confusion matrix:\n",
            " [[26  6]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [83.92857142857143, 94.64285714285714, 78.57142857142857, 92.72727272727273, 81.48148148148148]\n",
            "Mean accuracy: 86.27032227032227\n",
            "Accuracy sd: 6.316348734639284\n",
            "Median accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            "[[143  21]\n",
            " [ 17  96]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 40, 'preprocess': <function preprocess_nb at 0x7f956ab320d0>, 'epochs': 50, 'lr': 0.01, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 3min 23s, sys: 1min 25s, total: 4min 48s\n",
            "Wall time: 7min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cHM4eA8baTw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wideband: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "M0u_9bunfu0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "c54df868-af46-4dcf-8bc8-c46e6e4da48c"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wblinear_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wblinear_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_lin_transform, wb_image_transform,\n",
        "                        4)\n",
        "wblinear_conv3.train_cnn(wb_conv3_model,\n",
        "                        wblinear_conv3_params)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 1.9237412214279175\n",
            "5 0.7132323384284973\n",
            "10 0.49597427248954773\n",
            "15 0.511766791343689\n",
            "20 0.5456759333610535\n",
            "25 0.6200505495071411\n",
            "30 0.5789965987205505\n",
            "35 0.5380486249923706\n",
            "----------------------------------------\n",
            "Accuracy: 82.14285714285714\n",
            "Confusion matrix:\n",
            " [[30  3]\n",
            " [ 7 16]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 1.3762626647949219\n",
            "5 0.3205008804798126\n",
            "10 0.21794237196445465\n",
            "15 0.3926255702972412\n",
            "20 0.32697996497154236\n",
            "25 0.2365534007549286\n",
            "30 0.277342289686203\n",
            "35 0.48258060216903687\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[33  0]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 5.270444393157959\n",
            "5 0.5664018392562866\n",
            "10 0.4141639173030853\n",
            "15 0.5024642944335938\n",
            "20 0.6440936923027039\n",
            "25 0.44342905282974243\n",
            "30 0.41946491599082947\n",
            "35 0.6324747800827026\n",
            "----------------------------------------\n",
            "Accuracy: 89.28571428571429\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 3.240144729614258\n",
            "5 0.5669205188751221\n",
            "10 0.43896910548210144\n",
            "15 0.3676925599575043\n",
            "20 0.38934361934661865\n",
            "25 0.3614359498023987\n",
            "30 0.2929297983646393\n",
            "35 0.4241549074649811\n",
            "----------------------------------------\n",
            "Accuracy: 89.0909090909091\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 5 17]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 5.956076622009277\n",
            "5 0.3453187644481659\n",
            "10 0.450811505317688\n",
            "15 0.4310014843940735\n",
            "20 0.33807358145713806\n",
            "25 0.32880935072898865\n",
            "30 0.30304792523384094\n",
            "35 0.3353125751018524\n",
            "----------------------------------------\n",
            "Accuracy: 85.18518518518519\n",
            "Confusion matrix:\n",
            " [[26  6]\n",
            " [ 2 20]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [82.14285714285714, 91.07142857142857, 89.28571428571429, 89.0909090909091, 85.18518518518519]\n",
            "Mean accuracy: 87.35521885521887\n",
            "Accuracy sd: 3.2381293552253227\n",
            "Median accuracy: 89.0909090909091\n",
            "Confusion matrix:\n",
            "[[153  11]\n",
            " [ 24  89]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_wb at 0x7f95de75f158>, 'epochs': 40, 'lr': 0.01, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 3min 23s, sys: 1min 49s, total: 5min 12s\n",
            "Wall time: 6min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XM33xRQLfzGP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Narrowband: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "UbgEoUTvf1_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "831bec5d-ab83-4d42-8b4e-b13f5d43521d"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nblinear_conv5_params = {\n",
        "    'bs': 40,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nblinear_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_lin_transform, nb_image_transform,\n",
        "                        4)\n",
        "nblinear_conv5.train_cnn(nb_conv5_model,\n",
        "                        nblinear_conv5_params)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6959387063980103\n",
            "5 0.5345904231071472\n",
            "10 0.49803662300109863\n",
            "15 0.4328426718711853\n",
            "20 0.4722351133823395\n",
            "25 0.42862582206726074\n",
            "30 0.5922678112983704\n",
            "35 0.39143985509872437\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6841724514961243\n",
            "5 0.47707176208496094\n",
            "10 0.3412485420703888\n",
            "15 0.24678385257720947\n",
            "20 0.2949652373790741\n",
            "25 0.2630749046802521\n",
            "30 0.20997415482997894\n",
            "35 0.31942254304885864\n",
            "----------------------------------------\n",
            "Accuracy: 89.28571428571429\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.8615005612373352\n",
            "5 0.5793596506118774\n",
            "10 0.5002931356430054\n",
            "15 0.5497934818267822\n",
            "20 0.5893297791481018\n",
            "25 0.38790178298950195\n",
            "30 0.3490601181983948\n",
            "35 0.31896063685417175\n",
            "----------------------------------------\n",
            "Accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            " [[26  7]\n",
            " [ 2 21]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6850127577781677\n",
            "5 0.5204092264175415\n",
            "10 0.3950972855091095\n",
            "15 0.39630958437919617\n",
            "20 0.33751556277275085\n",
            "25 0.3182382881641388\n",
            "30 0.32657408714294434\n",
            "35 0.3072168827056885\n",
            "----------------------------------------\n",
            "Accuracy: 89.0909090909091\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 5 17]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7058238387107849\n",
            "5 0.5972446203231812\n",
            "10 0.4627809524536133\n",
            "15 0.3711410462856293\n",
            "20 0.5525298714637756\n",
            "25 0.43454334139823914\n",
            "30 0.309688538312912\n",
            "35 0.35329174995422363\n",
            "----------------------------------------\n",
            "Accuracy: 87.03703703703704\n",
            "Confusion matrix:\n",
            " [[30  2]\n",
            " [ 5 17]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [91.07142857142857, 89.28571428571429, 83.92857142857143, 89.0909090909091, 87.03703703703704]\n",
            "Mean accuracy: 88.08273208273208\n",
            "Accuracy sd: 2.439115521299596\n",
            "Median accuracy: 89.0909090909091\n",
            "Confusion matrix:\n",
            "[[152  12]\n",
            " [ 21  92]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 40, 'preprocess': <function preprocess_nb at 0x7f956ab320d0>, 'epochs': 40, 'lr': 0.01, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 2min 29s, sys: 54.9 s, total: 3min 24s\n",
            "Wall time: 4min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6TLUcfagHh_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wideband: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "LdlWILfNgPuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "f85e2dc9-a90b-4f51-df16-119d4f64c19a"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wblinear_conv5_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.005,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wblinear_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_lin_transform, wb_image_transform,\n",
        "                        4)\n",
        "wblinear_conv5.train_cnn(wb_conv5_model,\n",
        "                        wblinear_conv5_params)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6719483733177185\n",
            "5 0.6349017024040222\n",
            "10 0.6142798662185669\n",
            "15 0.49121856689453125\n",
            "20 0.4880121350288391\n",
            "25 0.5019400119781494\n",
            "30 0.49224525690078735\n",
            "35 0.5059398412704468\n",
            "----------------------------------------\n",
            "Accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            " [[29  4]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6908284425735474\n",
            "5 0.6124641299247742\n",
            "10 0.5082046985626221\n",
            "15 0.35303860902786255\n",
            "20 0.32355424761772156\n",
            "25 0.3409222662448883\n",
            "30 0.30748969316482544\n",
            "35 0.2892198860645294\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[31  2]\n",
            " [ 3 20]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6846070885658264\n",
            "5 0.5426108837127686\n",
            "10 0.5800987482070923\n",
            "15 0.53333979845047\n",
            "20 0.40505436062812805\n",
            "25 0.4527069926261902\n",
            "30 0.38134685158729553\n",
            "35 0.416520357131958\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[31  2]\n",
            " [ 3 20]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6556587219238281\n",
            "5 0.4913281798362732\n",
            "10 0.44747626781463623\n",
            "15 0.4149341881275177\n",
            "20 0.3705734610557556\n",
            "25 0.3816605508327484\n",
            "30 0.322396457195282\n",
            "35 0.3574160635471344\n",
            "----------------------------------------\n",
            "Accuracy: 92.72727272727273\n",
            "Confusion matrix:\n",
            " [[33  0]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.671930730342865\n",
            "5 0.5215020775794983\n",
            "10 0.34600919485092163\n",
            "15 0.48282620310783386\n",
            "20 0.33043020963668823\n",
            "25 0.3871283233165741\n",
            "30 0.3239019811153412\n",
            "35 0.2983926832675934\n",
            "----------------------------------------\n",
            "Accuracy: 88.88888888888889\n",
            "Confusion matrix:\n",
            " [[31  1]\n",
            " [ 5 17]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [83.92857142857143, 91.07142857142857, 91.07142857142857, 92.72727272727273, 88.88888888888889]\n",
            "Mean accuracy: 89.53751803751804\n",
            "Accuracy sd: 3.0581464931347058\n",
            "Median accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            "[[155   9]\n",
            " [ 20  93]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_wb at 0x7f95de75f158>, 'epochs': 40, 'lr': 0.005, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 3min 8s, sys: 1min 29s, total: 4min 38s\n",
            "Wall time: 6min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cDNJMdL9bhR5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mel-scale STFT spectrograms"
      ]
    },
    {
      "metadata": {
        "id": "kvKk3_UBbrFR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`librosa.features.melspectrogram()` was used to extract a  [mel scaled](https://en.wikipedia.org/wiki/Mel_scale)  spectrogram by applying a mel filterbank of a specified size to the STFT frequency bin values.\n",
        "\n",
        "128 mel bands were used for narrowband spectrograms and 512 for wideband spectrograms. As with linear spectrograms values were log scaled and normalised to $[-1, 1]$. Images were resized to 37$\\times$50 pixels for narrowband spectrograms and 154$\\times$12 pixels for wideband spectrograms with Lanczos resampling"
      ]
    },
    {
      "metadata": {
        "id": "n0ZZndpAeemH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MelSpectrogram(object):\n",
        "    \"\"\"\n",
        "    Creates a spectrogram from a raw audio signal using librosa\n",
        "    This isn't ideal but...\n",
        "    Args:\n",
        "        sr: sample rate\n",
        "        n_fft: size of fft\n",
        "    \"\"\"\n",
        "    def __init__(self, n_fft, n_mels, sr=16000, hop_length=None):\n",
        "        self.n_fft = n_fft\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length if hop_length else self.n_fft // 2\n",
        "        self.n_mels = n_mels\n",
        "    \n",
        "    def __call__(self, sig):\n",
        "        stft = np.abs(librosa.feature.melspectrogram(sig, n_fft=self.n_fft,\n",
        "                                                     hop_length=self.hop_length,\n",
        "                                                     n_mels=self.n_mels))\n",
        "        spectrogram = librosa.amplitude_to_db(stft, ref=np.max)\n",
        "        return spectrogram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WL2ld3NHf5D5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Parameters and transformations for narrowband mel STFT features\n",
        "\"\"\"\n",
        "nb_mel_params = {\n",
        "    'n_fft': 512,\n",
        "    'n_mels': 128\n",
        "}\n",
        "\n",
        "nb_mel_transform_librosa = transforms.Compose([\n",
        "    MelSpectrogram(n_fft=nb_mel_params['n_fft'],\n",
        "                  n_mels=nb_mel_params['n_mels'])\n",
        "])\n",
        "\n",
        "nb_mel_transform = transforms.Compose([\n",
        "    torchaudio.transforms.MelSpectrogram(ws=nb_mel_params['n_fft'],\n",
        "                                         n_mels=nb_mel_params['n_mels'])\n",
        "])\n",
        "\n",
        "nb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((37, 50), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XsitMRWzgy4g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Parameters and transformations for wideband mel STFT features\n",
        "\"\"\"\n",
        "wb_mel_params = {\n",
        "    'n_fft': 2048,\n",
        "    'n_mels': 512\n",
        "}\n",
        "\n",
        "wb_mel_transform_librosa = transforms.Compose([\n",
        "    MelSpectrogram(n_fft=wb_mel_params['n_fft'],\n",
        "                  n_mels=wb_mel_params['n_mels'])\n",
        "])\n",
        "\n",
        "wb_mel_transform = transforms.Compose([\n",
        "    torchaudio.transforms.MelSpectrogram(n_fft=wb_mel_params['n_fft'],\n",
        "                                         n_mels=wb_mel_params['n_mels'])\n",
        "])\n",
        "\n",
        "wb_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((154, 12), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tli0ElP2f3U1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Narrowband: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "1VaCFJbQgiiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "335c2b2c-90a0-49fe-9e88-40c372d1dcd7"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nbmel_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nbmel_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_mel_transform, nb_image_transform,\n",
        "                        4)\n",
        "nbmel_conv3.train_cnn(nb_conv3_model,\n",
        "                        nbmel_conv3_params)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7016878128051758\n",
            "5 0.6243820786476135\n",
            "10 0.49423959851264954\n",
            "15 0.4847390949726105\n",
            "20 0.4184781610965729\n",
            "25 0.4872109591960907\n",
            "30 0.4366031587123871\n",
            "35 0.4289983808994293\n",
            "----------------------------------------\n",
            "Accuracy: 85.71428571428571\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 7 16]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.851533830165863\n",
            "5 0.6193271279335022\n",
            "10 0.4264402985572815\n",
            "15 0.41130760312080383\n",
            "20 0.34002217650413513\n",
            "25 0.34428414702415466\n",
            "30 0.25307944416999817\n",
            "35 0.2608503997325897\n",
            "----------------------------------------\n",
            "Accuracy: 89.28571428571429\n",
            "Confusion matrix:\n",
            " [[33  0]\n",
            " [ 6 17]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6815876960754395\n",
            "5 0.648624837398529\n",
            "10 0.40572816133499146\n",
            "15 0.3922892212867737\n",
            "20 0.3856714069843292\n",
            "25 0.37773093581199646\n",
            "30 0.37458422780036926\n",
            "35 0.3590841293334961\n",
            "----------------------------------------\n",
            "Accuracy: 89.28571428571429\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7522616982460022\n",
            "5 0.570425271987915\n",
            "10 0.3631483316421509\n",
            "15 0.3242304027080536\n",
            "20 0.24946816265583038\n",
            "25 0.3249458372592926\n",
            "30 0.2709040939807892\n",
            "35 0.2704686224460602\n",
            "----------------------------------------\n",
            "Accuracy: 90.9090909090909\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6759218573570251\n",
            "5 0.7525907158851624\n",
            "10 0.4554731249809265\n",
            "15 0.37785953283309937\n",
            "20 0.4902316927909851\n",
            "25 0.35233429074287415\n",
            "30 0.3325430452823639\n",
            "35 0.3491041362285614\n",
            "----------------------------------------\n",
            "Accuracy: 90.74074074074075\n",
            "Confusion matrix:\n",
            " [[32  0]\n",
            " [ 5 17]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [85.71428571428571, 89.28571428571429, 89.28571428571429, 90.9090909090909, 90.74074074074075]\n",
            "Mean accuracy: 89.1871091871092\n",
            "Accuracy sd: 1.8686327011809634\n",
            "Median accuracy: 89.28571428571429\n",
            "Confusion matrix:\n",
            "[[161   3]\n",
            " [ 27  86]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_nb at 0x7f956ab320d0>, 'epochs': 40, 'lr': 0.01, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 2min 37s, sys: 1min 9s, total: 3min 46s\n",
            "Wall time: 5min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FrAupjL_gwlz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wideband: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "bcX-iMnwgo-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "963a8127-8f1c-45eb-b349-a3c789741c92"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wbmel_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.01,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wbmel_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_mel_transform, wb_image_transform,\n",
        "                        4)\n",
        "wbmel_conv3.train_cnn(wb_conv3_model,\n",
        "                        wbmel_conv3_params)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6971888542175293\n",
            "5 0.6152015924453735\n",
            "10 0.47182053327560425\n",
            "15 0.4439582824707031\n",
            "20 0.4474715292453766\n",
            "25 0.4320997893810272\n",
            "30 0.4457048773765564\n",
            "35 0.4560348689556122\n",
            "----------------------------------------\n",
            "Accuracy: 80.35714285714286\n",
            "Confusion matrix:\n",
            " [[26  7]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 1.6278432607650757\n",
            "5 0.6013935208320618\n",
            "10 0.3782952129840851\n",
            "15 0.3012264668941498\n",
            "20 0.2660313546657562\n",
            "25 0.2242439091205597\n",
            "30 0.5055945515632629\n",
            "35 0.2700257897377014\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.8375638127326965\n",
            "5 0.47018125653266907\n",
            "10 0.4296555817127228\n",
            "15 0.38690462708473206\n",
            "20 0.4246788024902344\n",
            "25 0.3984873294830322\n",
            "30 0.4835050702095032\n",
            "35 0.37092718482017517\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6900436282157898\n",
            "5 0.5086709260940552\n",
            "10 0.31104686856269836\n",
            "15 0.29813769459724426\n",
            "20 0.4007190465927124\n",
            "25 0.29959192872047424\n",
            "30 0.2730060815811157\n",
            "35 0.3095400631427765\n",
            "----------------------------------------\n",
            "Accuracy: 85.45454545454545\n",
            "Confusion matrix:\n",
            " [[27  6]\n",
            " [ 2 20]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6883708834648132\n",
            "5 0.5839194059371948\n",
            "10 0.3525558412075043\n",
            "15 0.3708551526069641\n",
            "20 0.3650408387184143\n",
            "25 0.32018929719924927\n",
            "30 0.4487226605415344\n",
            "35 0.31849613785743713\n",
            "----------------------------------------\n",
            "Accuracy: 75.92592592592592\n",
            "Confusion matrix:\n",
            " [[25  7]\n",
            " [ 6 16]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [80.35714285714286, 91.07142857142857, 91.07142857142857, 85.45454545454545, 75.92592592592592]\n",
            "Mean accuracy: 84.77609427609427\n",
            "Accuracy sd: 5.959452579741539\n",
            "Median accuracy: 85.45454545454545\n",
            "Confusion matrix:\n",
            "[[142  22]\n",
            " [ 20  93]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_wb at 0x7f95de75f158>, 'epochs': 40, 'lr': 0.01, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 4min 35s, sys: 2min 27s, total: 7min 3s\n",
            "Wall time: 8min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dbgBTmy7gt3s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Narrowband: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "9O64DcP-gwne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "13eb7348-4a0f-4916-b3dc-92e902349197"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "nbmel_conv5_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.001,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "nbmel_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        nb_mel_transform, nb_image_transform,\n",
        "                        4)\n",
        "nbmel_conv5.train_cnn(nb_conv5_model,\n",
        "                        nbmel_conv5_params)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6626893281936646\n",
            "5 0.6466760635375977\n",
            "10 0.496099054813385\n",
            "15 0.49459704756736755\n",
            "20 0.4645806849002838\n",
            "25 0.47265729308128357\n",
            "30 0.44997552037239075\n",
            "35 0.4452265799045563\n",
            "----------------------------------------\n",
            "Accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            " [[28  5]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6648707389831543\n",
            "5 0.531322181224823\n",
            "10 0.40431123971939087\n",
            "15 0.3214481472969055\n",
            "20 0.31559258699417114\n",
            "25 0.398965984582901\n",
            "30 0.383230596780777\n",
            "35 0.2972760498523712\n",
            "----------------------------------------\n",
            "Accuracy: 94.64285714285714\n",
            "Confusion matrix:\n",
            " [[31  2]\n",
            " [ 1 22]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6682575345039368\n",
            "5 0.611322820186615\n",
            "10 0.5085105895996094\n",
            "15 0.4536117911338806\n",
            "20 0.419877827167511\n",
            "25 0.48042258620262146\n",
            "30 0.48438772559165955\n",
            "35 0.4870491027832031\n",
            "----------------------------------------\n",
            "Accuracy: 75.0\n",
            "Confusion matrix:\n",
            " [[21 12]\n",
            " [ 2 21]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7799111008644104\n",
            "5 0.6389373540878296\n",
            "10 0.43893712759017944\n",
            "15 0.32797396183013916\n",
            "20 0.34055766463279724\n",
            "25 0.27758410573005676\n",
            "30 0.3059319257736206\n",
            "35 0.3257702589035034\n",
            "----------------------------------------\n",
            "Accuracy: 83.63636363636364\n",
            "Confusion matrix:\n",
            " [[26  7]\n",
            " [ 2 20]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6854767799377441\n",
            "5 0.6552441716194153\n",
            "10 0.4379623532295227\n",
            "15 0.42314931750297546\n",
            "20 0.3984554409980774\n",
            "25 0.39314308762550354\n",
            "30 0.42032650113105774\n",
            "35 0.39483916759490967\n",
            "----------------------------------------\n",
            "Accuracy: 85.18518518518519\n",
            "Confusion matrix:\n",
            " [[29  3]\n",
            " [ 5 17]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [83.92857142857143, 94.64285714285714, 75.0, 83.63636363636364, 85.18518518518519]\n",
            "Mean accuracy: 84.47859547859548\n",
            "Accuracy sd: 6.2396656540485\n",
            "Median accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            "[[135  29]\n",
            " [ 14  99]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_nb at 0x7f956ab320d0>, 'epochs': 40, 'lr': 0.001, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 2min 27s, sys: 56 s, total: 3min 23s\n",
            "Wall time: 4min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GHI1GYbigzmy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Wideband: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "DAvS3hWrg1gZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "73455c73-129e-4faa-8809-17793c3a50b5"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "wbmel_conv5_params = {\n",
        "    'bs': 40,\n",
        "    'preprocess': preprocess_wb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.001,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "wbmel_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        wb_mel_transform, wb_image_transform,\n",
        "                        4)\n",
        "wbmel_conv5.train_cnn(wb_conv5_model,\n",
        "                        wbmel_conv5_params)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.708220362663269\n",
            "5 0.6587188839912415\n",
            "10 0.5785630345344543\n",
            "15 0.5322193503379822\n",
            "20 0.582286536693573\n",
            "25 0.5688413381576538\n",
            "30 0.5906766653060913\n",
            "35 0.5250605344772339\n",
            "----------------------------------------\n",
            "Accuracy: 82.14285714285714\n",
            "Confusion matrix:\n",
            " [[27  6]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7202353477478027\n",
            "5 0.6716393828392029\n",
            "10 0.6019241213798523\n",
            "15 0.48508432507514954\n",
            "20 0.36403751373291016\n",
            "25 0.34811872243881226\n",
            "30 0.3455410301685333\n",
            "35 0.4421089291572571\n",
            "----------------------------------------\n",
            "Accuracy: 91.07142857142857\n",
            "Confusion matrix:\n",
            " [[33  0]\n",
            " [ 5 18]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.656867504119873\n",
            "5 0.6330108642578125\n",
            "10 0.5020360350608826\n",
            "15 0.5446205735206604\n",
            "20 0.5630818605422974\n",
            "25 0.5834950804710388\n",
            "30 0.5708104372024536\n",
            "35 0.5613563656806946\n",
            "----------------------------------------\n",
            "Accuracy: 76.78571428571429\n",
            "Confusion matrix:\n",
            " [[24  9]\n",
            " [ 4 19]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 1.2452425956726074\n",
            "5 0.6424316167831421\n",
            "10 0.4797508120536804\n",
            "15 0.4167679250240326\n",
            "20 0.4382542371749878\n",
            "25 0.3695695102214813\n",
            "30 0.382820188999176\n",
            "35 0.35988956689834595\n",
            "----------------------------------------\n",
            "Accuracy: 87.27272727272727\n",
            "Confusion matrix:\n",
            " [[30  3]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.670043408870697\n",
            "5 0.6515952944755554\n",
            "10 0.5230714082717896\n",
            "15 0.43082568049430847\n",
            "20 0.40351879596710205\n",
            "25 0.4038293957710266\n",
            "30 0.3814692795276642\n",
            "35 0.3535396158695221\n",
            "----------------------------------------\n",
            "Accuracy: 92.5925925925926\n",
            "Confusion matrix:\n",
            " [[32  0]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [82.14285714285714, 91.07142857142857, 76.78571428571429, 87.27272727272727, 92.5925925925926]\n",
            "Mean accuracy: 85.97306397306397\n",
            "Accuracy sd: 5.8408643890252225\n",
            "Median accuracy: 87.27272727272727\n",
            "Confusion matrix:\n",
            "[[146  18]\n",
            " [ 21  92]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 40, 'preprocess': <function preprocess_wb at 0x7f95de75f158>, 'epochs': 40, 'lr': 0.001, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 4min 5s, sys: 1min 57s, total: 6min 3s\n",
            "Wall time: 7min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aMiXtS1u8V5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mel Frequency Cepstral Coefficients (MFCCs)"
      ]
    },
    {
      "metadata": {
        "id": "tWRIYAq-k4L4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Along with the four spectral features, mel-frequency cepstral coefficients were also extracted from the raw audio. Per Huzaifah (2017: 2) these were 'obtained using the standard procedure'. That is by:\n",
        "Huzaifah (2017: 2) writes that 'MFCCs were obtained using the standard procedure'. That is by:\n",
        "\n",
        "1. Computing the STFT of the signal\n",
        "2. Applying a mel filterbank to the power spectrum of the signal, producing the mel-scaled STFT\n",
        "3. Taking the logarithm of the powers of each mel frequency\n",
        "4. Taking the Discrete Cosine Transform of the list of log mel powers.\n",
        "\n",
        "There is no implementation of MFCCs in `torchaudio`. As a result, `librosa` was used here. The default behaviour of `librosa.feature.mfcc` is to only return the first 20 MFCCs. This was left unchanged as it was assumed to be the approach used by Huzaifah. MFCC values were normalised to $[-1, 1]$ and the image resized to 37$\\times$50 pixels with Lanczos resampling."
      ]
    },
    {
      "metadata": {
        "id": "Z30Jfr-7oimd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MFCC(object):\n",
        "    \"\"\"\n",
        "    Gets MFCCs from a raw audio signal using librosa\n",
        "    Args:\n",
        "        sr: sample rate\n",
        "        n_fft: size of fft\n",
        "    \"\"\"\n",
        "    def __init__(self, sr=16000, n_mfcc=20):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "    \n",
        "    def __call__(self, sig):\n",
        "        mfcc = librosa.feature.mfcc(sig, sr=self.sr, n_mfcc=self.n_mfcc)\n",
        "        return mfcc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TD8XEiHpp2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mfcc_transform = transforms.Compose([\n",
        "    MFCC()\n",
        "])\n",
        "\n",
        "mfcc_image_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((37, 50), 4),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSEJ5DD3p0It",
        "colab_type": "code",
        "outputId": "fb21a1a4-3bd2-410c-c5d4-140c2b86a179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "cell_type": "code",
      "source": [
        "mfcc = AudioFeatureDataset(data_df, ROOT_PATH,\n",
        "                                    audio_transform=mfcc_transform,\n",
        "                                    image_transform=mfcc_image_transform,\n",
        "                                    duration=4, librosa=True)\n",
        "image_mfcc = mfcc.__getitem__(15)[0]\n",
        "\n",
        "plt.imshow(image_mfcc[0], origin='lower')\n",
        "plt.axis('off')\n",
        "plt.title('MFCCs')\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAFZCAYAAADw7toaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEqdJREFUeJzt3UuIXvX5B/BnonNJHG+JmqDVKBIF\na0GLGBRBqHfdq7R1Z6suilIFFwXpQsGCIkpQFBRRCy6UglSshkIREdGFSqglVRARetHaJl4yzkyS\n+a+U+jfze47ze2eSPP18ds4z537O+/WNOX7HFhYWFgIADnKr9vcOAMAoCDQAShBoAJQg0AAoQaAB\nUIJAA6CEQ/f3DsDB7PTTT4/LLrssHnjggW/8/Fe/+lU888wzsX379q9/76STTopDDjnk69854YQT\n4tFHH42IiLfffjvuu++++Nvf/hZ79+6NE044IW6++eb44Q9/GBERCwsL8fjjj8ezzz4b8/PzsWfP\nnrjgggvi1ltvjcMPP3yFjhYObAINOm3fvj0+//zzmJ6ejoiIubm52LZt27d+78knn4wNGzZ86+d/\n+ctf4uc//3ncddddcfHFF0dExB//+Mf42c9+Fk8//XRs2rQp7rnnnnj99dfj0UcfjfXr18euXbvi\nrrvuihtuuCF++9vfxtjY2PIeJBwE/JEjdNq8eXNs3br1639+5ZVX4gc/+MHg5R966KG45pprvg6z\niIiLLrootmzZEuvWrYsdO3bEk08+GXfffXesX78+IiLWrFkTd9xxR1x//fWxsLAQf/3rX+Oaa66J\nq666Ki699NJ46qmnRneAcJAQaNDpiiuuiN///vdf//Pzzz8fl19++eDl33jjjbjwwgu/9fPzzjsv\n1q5dG2+//XZs2LAhTj311G/MJycn40c/+lGsWrUqtmzZEtdee208//zz8fTTT8err74ac3NzSz8o\nOAj5I0fodO6558btt98en3zySaxZsybefPPN+M1vfvOt37vuuuu+8d/QzjnnnLjzzjtj586dccwx\nxyy6/h07dsS6deua+7Bu3bp48cUX47TTToszzjgjHnzwwaUfEBykBBp0OuSQQ+LSSy+NF154Idau\nXRsXXHBBHHrotx+txf4b2tFHHx3//Oc/Y+PGjftc/1fzlttuuy0efvjhuOWWW2J2djZuuOGG+MlP\nfrK0A4KDlD9yhBG48sor48UXX4w//OEPceWVV36nZTdv3hwvvfTSt37+7LPPxrZt2+Kss86KTz75\nJP785z9/Yz4/Px/33XdfzMzMxGGHHRa//OUvY+vWrbFly5Z44IEH4v333+86JjjYCDQYgbPPPjs+\n+uijePfdd+Pcc8/9TsvedNNN8dxzz8Xvfve7r3+2devWuPfee2N6ejqOOOKIuP766+P222+PDz74\nICIiZmZm4o477oh33nknVq9eHTfeeGO8++67ERFx2mmnxfT0tL/5yP8cf+QIIzA2NhaXXHJJzMzM\nxKpV3+3fEzdt2hSPPfZY3HvvvbFly5aYmJiIjRs3xuOPPx6nnHJKRET84he/iCOPPDJuuumm2LNn\nT6xatSouuuii+PWvfx0RET/96U/j1ltvjfn5+YiI+PGPfxwnn3zyKA8RDnhj+tAAqMAfOQJQgkAD\noASBBkAJAg2AEgQaACWsyF/br/AXKbNj2LNnT9d8YmLiO+/Tf8veOcr2f9euXc35xx9/3JxPTU01\n5xHR/N87DfFd/zr8/5edo2y+d+/eru3/9//2al+GPCfZPvSeowNd7zWIyM9Rdh2yebb+7P+xua//\ny8x/y85BtvwozM7ONuc7d+5szrPPguwYx8fH9/nz2nc/AP8zBBoAJQg0AEoQaACUINAAKEGgAVCC\nQAOgBIEGQAkCDYASBBoAJQg0AEoQaACUINAAKEGgAVCCQAOghBXpQ1tuWXdONs96qobI1tHbl5bZ\nvXt3c97bhzY9Pd2cr127tjmPiJifn2/OF+s4+spSO5K+0tvLl/WlZXp7tiKWvxOudx97158ZRd9b\nby9e9qxlz3LvOez9vMr2f0jnXNZ3lu1j77O0GN/QAChBoAFQgkADoASBBkAJAg2AEgQaACUINABK\nEGgAlCDQAChBoAFQgkADoASBBkAJAg2AEgQaACUINABKWJE+tN4+smz55e6Iisg7jLKur+wYs/XP\nzc0157Ozs815dg6zc3DUUUc159nxR+TXqfc67u++s8wo7uP93UfWu/7e3r9DD+3/yOrdh95zmN0H\n2fqz5bPPit4+uIi8e3BqaqprG0vtfPMNDYASBBoAJQg0AEoQaACUINAAKEGgAVCCQAOgBIEGQAkC\nDYASBBoAJQg0AEoQaACUINAAKEGgAVCCQAOgBIEGQAljC71tdQNkhXSZ3lLBUZQeZuvYvXt3c54V\n1vWuPystzI4xK+ybmZlpzoeUAk5MTHTtw3KXU2bnOCvg7C1xHSLbx+wcZvdJ7zEsdxnvSpSkZrL7\nKNvH7BpmJaa9n2e9ZcQR/ceQWeqz5hsaACUINABKEGgAlCDQAChBoAFQgkADoASBBkAJK/IeWvbe\nQ+87CweC3vdzsveDsndPsvlyv1805Bpm+5jdJ5nsGLJ3W3rP8ZB38Xr1Pq695yiz3O9jjkLvNrJz\nmD3LvbL1Z9cwWz57l3GI7BwNedetZbFr6BsaACUINABKEGgAlCDQAChBoAFQgkADoASBBkAJAg2A\nEgQaACUINABKEGgAlCDQAChBoAFQgkADoASBBkAJK9KHlm0i61Ba7r60rOtriN4+sayDKFu+t+Mp\n6yLLOpR6+42GbGNycrJr/dk+9t4H2TXI7pGV6ALr7dJa7j61A0HvdZqbm2vOs8+z/X0fDXkOst/p\nfZayZ3Wxz8uD/+4DgBBoABQh0AAoQaABUIJAA6AEgQZACQINgBIEGgAlCDQAShBoAJQg0AAoQaAB\nUIJAA6AEgQZACQINgBJWpA+tt8sr60vLZN06Q05Bdgy9HUfZMfZ2HPV2LPUe/8Ggt2dqJfrOsuvQ\n20d2IHS2tRzo+xeRP8u9nXK9vX6j6KzLtpH17vV+Xix2DL6hAVCCQAOgBIEGQAkCDYASBBoAJQg0\nAEoQaACUINAAKEGgAVCCQAOgBIEGQAkCDYASBBoAJQg0AEoQaACUcED0ofXuQrZ8Nh/SzZP1+/R2\nrvX2OGXnOOto6u1QGtKx1NuB1Hudl1t2DrN7ZCW6vLJt7O+eq97Piuwcj0K2D/Pz881572dFtv5R\n9J31Gh8fb85778PF7rP9f+QAMAICDYASBBoAJQg0AEoQaACUINAAKEGgAVCCQAOgBIEGQAkCDYAS\nBBoAJQg0AEoQaACUINAAKEGgAVDCivShZR1KmeXu9xmyf71dXFn/T2/fWHYMvdsfRRdZto25ubnm\nPOva6u1Y6u2pWu5ev4j+69jb69fbV9bb+TaKzrhsH5f7Wci6CbNj7O1Dy65x1mUW0X8OZmZmmvPJ\nycnmfLF99A0NgBIEGgAlCDQAShBoAJQg0AAoQaABUIJAA6AEgQZACQINgBIEGgAlCDQAShBoAJQg\n0AAoQaABUIJAA6CEdsHUiPT2C2UdTllHU9ajNaRvbRQ9TC3ZMWTnIDuHWQdS1tGUdTBly0fkxzAx\nMdGcZ9cpO4fZ+jO9fWuZIb18vfuQzff3fZ7J9m8U+5/dy9nnSe8xZvd5dh9n5yDb/yG9fNnvzM7O\nNue9vX6LLrekpQDgACPQAChBoAFQgkADoASBBkAJAg2AEgQaACWMLQx56aBTtolsnr2zkL2/k72D\nNUTvuym95yB7t6X3Xb/lXj4iP4e975llsvUv9d2Xr/Tex0P0nsPec5w9S73vU2Z679OI/uuQbSN7\nZzPbfu/nVbZ8dg1G8T5k73241HdWfUMDoASBBkAJAg2AEgQaACUINABKEGgAlCDQAChBoAFQgkAD\noASBBkAJAg2AEgQaACUINABKEGgAlCDQAChBoAFQwgFR8NkrKz3MyuLGx8fTbWTFhVnh3SgKMnv0\nrj87h3Nzc+k6suLBrCS1t+S0t6S1d/3Z8Q8pnux9lnrv0971Z3qv8ZAS2N5yyt6S0t5jXO4i2lF8\nXmclp4sVdA7dh8WeJd/QAChBoAFQgkADoASBBkAJAg2AEgQaACUINABKEGgAlCDQAChBoAFQgkAD\noASBBkAJAg2AEgQaACUINABKaBdAjcjs7Gxz3tuhlBlF/0/W75N1FGVdW5neDqTsGmQdT9n+T01N\npfvQ2yOVXYOsbyyT9VQtd1fXkOcg+53ec5xd5+w+ys7BUnuuvtJ7jYZsIzuH2Tay9Wf38eTkZNfy\nvb18Qz4Ps/uwt+9sqZ1svqEBUIJAA6AEgQZACQINgBIEGgAlCDQAShBoAJQg0AAoQaABUIJAA6AE\ngQZACQINgBIEGgAlCDQAShBoAJSwIn1oWf9O1sG01G6cryx3h1RE3kGUzcfHx5vzrGcq66nK5tn2\nR9FflF2H3vuk9xiy/cvm2fpH0eXV27mW9VRlsuWXuyssu8ZD9F7HzKefftqcZ+cwe1Yz2TWYm5tr\nzod0L/Z+Zmf7mHXCLcY3NABKEGgAlCDQAChBoAFQgkADoASBBkAJAg2AEgQaACUINABKEGgAlCDQ\nAChBoAFQgkADoASBBkAJAg2AEsYWest/BpiZmWnOs26crHsn8+WXX3ZtPyJi9erVzXnW85TtQ9b/\n09tH1nuOsz63IbJj7N1Gdg6y9WfnoLfrK9u/IT1UWXffcne2Zeeo9xxk1yjrChtyDrNOtd77JLvP\ns8637Bpm1yhbPjMkErJzlN0H2TnatWtXcz49Pb3Pn/uGBkAJAg2AEgQaACUINABKEGgAlCDQAChB\noAFQgkADoASBBkAJAg2AEgQaACUINABKEGgAlCDQAChBoAFQQl/R2EBZv05vh1O2fNZfNKRvLev3\nyTrfsm30HkNvh1Jvz1TW0zVEto8TExPNeXYNerefXYPlvs+HrCObZ/fxcncT9l7jbD6kUy+7V7O+\ntGz5rO8se5Z6l8/2fxS9fL33WWap95lvaACUINAAKEGgAVCCQAOgBIEGQAkCDYASBBoAJQg0AEoQ\naACUINAAKEGgAVCCQAOgBIEGQAkCDYASBBoAJYwtDClh6vTpp592LZ9142QdS1l3z5Aur6mpqa5t\nfPHFF13LZ/uYdRj19mhlPVRDzmHWV3bkkUc251lnW28fWSY7x9k8u0+zeUR+jKPoumr57LPPmvPs\nWc3uo96+tSEfZ9m92vusZceYXefVq1c3573dhaN4lpf7Wcgs1rfmGxoAJQg0AEoQaACUINAAKEGg\nAVCCQAOgBIEGQAkCDYAS+t5iHCh7EXBubq5rvnv37uZ8/fr1zfn8/HxzHpG/CPjxxx8359nLitnL\nlIcffnjX+rMXu//1r38158ccc0xznr00HRHx0UcfNecnnXRSc75hw4bmPHspN3spOLtPv/zyy67t\nZy/9DjmH2Uux2Tx7lrKX248++ujmPHuWep+T7OX48fHx5jwiv07ZPqxdu7Y5z/bx9ddfb86z+/Dk\nk09uzntfTh9i165dzXl2H2Sfyccff/x33qcI39AAKEKgAVCCQAOgBIEGQAkCDYASBBoAJQg0AEoQ\naACUINAAKEGgAVCCQAOgBIEGQAkCDYASBBoAJQg0AEpYkT60J554ojk/4ogjmvPTTz+9Oc+6wv7z\nn/8051lPVkTE1NRUc37cccc156ecckpznnUoZR1JWZfWUUcd1ZyvW7euOV+1qv3vPpOTk835EL1d\nWR9++GFznh3Dzp07m/PsGmf3YdZFlt1jERHvvfde1z5knWxvvvlmc56dw+w++973vtecZz1ab731\nVnO+Y8eO5jwi70/MnsXzzjuvOc+6B7NzfPHFFzfnJ554YnO+Zs2a5jz7LMmes4i8dy/rS9u+fXtz\nnnUPbtq0aZ8/9w0NgBIEGgAlCDQAShBoAJQg0AAoQaABUIJAA6AEgQZACQINgBIEGgAlCDQAShBo\nAJQg0AAoQaABUIJAA6CEFelDy3qqsh6srDsn60s7//zzm/N///vfzXlExJ/+9Kfm/MYbb2zOsw6i\nrKMp62z7xz/+0ZxnPVbj4+PN+TvvvNOcD+lDO+ecc5rz7BgfeeSR5vyKK65ozp955pnm/LXXXmvO\nv//97zfnl112WXO+evXq5jzrAouIeOWVV5rzrI8su8+2bdvWnG/evLk5P/bYY5vz7BizPre///3v\nzflzzz3XnEdEvPzyy835mWee2Zxn3YNPPfVUc3711Vc35xs3bmzO9+7d25xnn7dDOuMyWcdkdq8f\nf/zxzfn999/fnN9zzz37/LlvaACUINAAKEGgAVCCQAOgBIEGQAkCDYASBBoAJQg0AEoQaACUINAA\nKEGgAVCCQAOgBIEGQAkCDYASBBoAJYwtLCws7O+dAIBevqEBUIJAA6AEgQZACQINgBIEGgAlCDQA\nShBoAJQg0AAoQaABUIJAA6AEgQZACQINgBIEGgAlCDQAShBoAJQg0AAoQaABUIJAA6AEgQZACQIN\ngBIEGgAlCDQAShBoAJTwf9/mXIXU6PgFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PxuiJj457q13",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MFCCs: Conv-3"
      ]
    },
    {
      "metadata": {
        "id": "JBntfzXe7tdb",
        "colab_type": "code",
        "outputId": "9b389d17-b371-44bd-e83e-6fb136059103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "mfcc_conv3_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.001,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "mfcc_conv3 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        mfcc_transform, nb_image_transform,\n",
        "                        4, librosa=True)\n",
        "mfcc_conv3.train_cnn(nb_conv3_model,\n",
        "               mfcc_conv3_params)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 1.706463098526001\n",
            "5 0.6292866468429565\n",
            "10 0.5988512635231018\n",
            "15 0.6176655888557434\n",
            "20 0.5740047097206116\n",
            "25 0.5852469205856323\n",
            "30 0.5508890151977539\n",
            "35 0.576517641544342\n",
            "----------------------------------------\n",
            "Accuracy: 76.78571428571429\n",
            "Confusion matrix:\n",
            " [[26  7]\n",
            " [ 6 17]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.5657273530960083\n",
            "5 0.44844862818717957\n",
            "10 0.4231627285480499\n",
            "15 0.41911596059799194\n",
            "20 0.4156605303287506\n",
            "25 0.4178639352321625\n",
            "30 0.5073944926261902\n",
            "35 0.4061567783355713\n",
            "----------------------------------------\n",
            "Accuracy: 83.92857142857143\n",
            "Confusion matrix:\n",
            " [[32  1]\n",
            " [ 8 15]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6490617394447327\n",
            "5 0.49041682481765747\n",
            "10 0.4691601097583771\n",
            "15 0.45807403326034546\n",
            "20 0.4474366009235382\n",
            "25 0.44558361172676086\n",
            "30 0.42596182227134705\n",
            "35 0.43565407395362854\n",
            "----------------------------------------\n",
            "Accuracy: 85.71428571428571\n",
            "Confusion matrix:\n",
            " [[31  2]\n",
            " [ 6 17]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 3.348752498626709\n",
            "5 0.39816173911094666\n",
            "10 0.3939043879508972\n",
            "15 0.41119638085365295\n",
            "20 0.3998579978942871\n",
            "25 0.40578019618988037\n",
            "30 0.4867824614048004\n",
            "35 0.4026559889316559\n",
            "----------------------------------------\n",
            "Accuracy: 80.0\n",
            "Confusion matrix:\n",
            " [[31  2]\n",
            " [ 9 13]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.5620830059051514\n",
            "5 0.49512842297554016\n",
            "10 0.45389530062675476\n",
            "15 0.45136696100234985\n",
            "20 0.46322396397590637\n",
            "25 0.41285088658332825\n",
            "30 0.41822847723960876\n",
            "35 0.4105520248413086\n",
            "----------------------------------------\n",
            "Accuracy: 83.33333333333333\n",
            "Confusion matrix:\n",
            " [[31  1]\n",
            " [ 8 14]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [76.78571428571429, 83.92857142857143, 85.71428571428571, 80.0, 83.33333333333333]\n",
            "Mean accuracy: 81.95238095238095\n",
            "Accuracy sd: 3.1771220166040206\n",
            "Median accuracy: 83.33333333333333\n",
            "Confusion matrix:\n",
            "[[151  13]\n",
            " [ 37  76]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_nb at 0x7f956ab320d0>, 'epochs': 40, 'lr': 0.001, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 18min 46s, sys: 14min 33s, total: 33min 19s\n",
            "Wall time: 17min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HJ6EQpGVhbn7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MFCCs: Conv-5"
      ]
    },
    {
      "metadata": {
        "id": "_kFFeYJGhdeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1696
        },
        "outputId": "0419d1ba-a400-4203-d936-ed4bec8993b6"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "mfcc_conv5_params = {\n",
        "    'bs': 30,\n",
        "    'preprocess': preprocess_nb,\n",
        "    'epochs': 40,\n",
        "    'lr': 0.001,\n",
        "    'l2': 0.005\n",
        "}\n",
        "\n",
        "mfcc_conv5 = CVModel(data_df, 5, ROOT_PATH,\n",
        "                        mfcc_transform, nb_image_transform,\n",
        "                        4, librosa=True)\n",
        "mfcc_conv5.train_cnn(nb_conv5_model,\n",
        "               mfcc_conv5_params)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.799129843711853\n",
            "5 0.6764841675758362\n",
            "10 0.659523069858551\n",
            "15 0.663834273815155\n",
            "20 0.649513840675354\n",
            "25 0.7301478385925293\n",
            "30 0.7419062852859497\n",
            "35 0.6018915176391602\n",
            "----------------------------------------\n",
            "Accuracy: 71.42857142857143\n",
            "Confusion matrix:\n",
            " [[24  9]\n",
            " [ 7 16]]\n",
            "----------------------------------------\n",
            "Fold 2 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7206292748451233\n",
            "5 0.6219107508659363\n",
            "10 0.4859585165977478\n",
            "15 0.45407241582870483\n",
            "20 0.4601070284843445\n",
            "25 0.5108188390731812\n",
            "30 0.5161507725715637\n",
            "35 0.5608971118927002\n",
            "----------------------------------------\n",
            "Accuracy: 87.5\n",
            "Confusion matrix:\n",
            " [[28  5]\n",
            " [ 2 21]]\n",
            "----------------------------------------\n",
            "Fold 3 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.6690703630447388\n",
            "5 0.5749691724777222\n",
            "10 0.48227277398109436\n",
            "15 0.4692835807800293\n",
            "20 0.4648383557796478\n",
            "25 0.4841438829898834\n",
            "30 0.47687405347824097\n",
            "35 0.48547911643981934\n",
            "----------------------------------------\n",
            "Accuracy: 82.14285714285714\n",
            "Confusion matrix:\n",
            " [[31  2]\n",
            " [ 8 15]]\n",
            "----------------------------------------\n",
            "Fold 4 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.8952519297599792\n",
            "5 0.6215709447860718\n",
            "10 0.43723148107528687\n",
            "15 0.4316132366657257\n",
            "20 0.38639339804649353\n",
            "25 0.3728916049003601\n",
            "30 0.35139229893684387\n",
            "35 0.3925642967224121\n",
            "----------------------------------------\n",
            "Accuracy: 81.81818181818181\n",
            "Confusion matrix:\n",
            " [[26  7]\n",
            " [ 3 19]]\n",
            "----------------------------------------\n",
            "Fold 5 / 5:\n",
            "Training the model on the GPU\n",
            "0 0.7021729946136475\n",
            "5 0.5726082921028137\n",
            "10 0.5039190649986267\n",
            "15 0.49236607551574707\n",
            "20 0.4629736542701721\n",
            "25 0.523077130317688\n",
            "30 0.496001273393631\n",
            "35 0.5099981427192688\n",
            "----------------------------------------\n",
            "Accuracy: 79.62962962962963\n",
            "Confusion matrix:\n",
            " [[25  7]\n",
            " [ 4 18]]\n",
            "----------------------------------------\n",
            "========================================\n",
            "Accuracy scores: [71.42857142857143, 87.5, 82.14285714285714, 81.81818181818181, 79.62962962962963]\n",
            "Mean accuracy: 80.503848003848\n",
            "Accuracy sd: 5.224650605927692\n",
            "Median accuracy: 81.81818181818181\n",
            "Confusion matrix:\n",
            "[[134  30]\n",
            " [ 24  89]]\n",
            "----------------------------------------\n",
            "Model parameters:\n",
            " {'bs': 30, 'preprocess': <function preprocess_nb at 0x7f956ab320d0>, 'epochs': 40, 'lr': 0.001, 'l2': 0.005}\n",
            "========================================\n",
            "CPU times: user 19min 48s, sys: 14min 55s, total: 34min 44s\n",
            "Wall time: 18min 23s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}