{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDYmGudeDjeP"
   },
   "source": [
    "# Classification of cat and dog vocalisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SGKShAduGHBX"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oMSr4cdvYto_"
   },
   "source": [
    "Training a machine learning classification model on images of cats and dogs is a common introductory problem. With modern convolutional neural network (CNN) architectures and transfer learning, it is now possible to achieve near-perfect levels of classification accuracy as demonstrated very effectively (and efficiently!) early on in the [fast.ai deep learning MOOC](https://course.fast.ai/). When browsing through the datasets on Kaggle some time ago, I stumbled upon the [Audio Cats and Dogs](https://www.kaggle.com/mmoreaux/audio-cats-and-dogs/home) data, which presents itself as 'the audio counterpart' to the typical image classification problem. As someone with a background in acoustic phonetics, this has rather a lot of appeal. In addition, the dataset brings some new challenges to classification:  first, the audio data requires considerable preprocessing if a CNN is to be used, and second, the data consists of only 277 files. More on both these topics later.\n",
    "\n",
    "This notebook comprises two major parts. The first is a partial implementation of the approaches described in Huzaifah (2017) in his experimentation regarding CNN-based classification of environmental sound data. While the CNN architectures he used were implemented in TensorFlow, I have chosen instead to use Pytorch. The second section ((improves on the approaches used by Huzaifah, increasing the overall classification accuracy using TODO: finish this once the implementation is done))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vC_TvzAZGKFy"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGq3XwxuG5DB"
   },
   "source": [
    "[Librosa](http://librosa.github.io/librosa/) is an excellent general-purpose audio processing and analysis library. The remainder of the libraries used in this project are standard in data science/ML and hopefully need no introduction. Pytorch-related libraries are loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ock2HWBkjzk"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np #Linear algebra\n",
    "import pandas as pd #Data frames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa as lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iobujbDyZP65"
   },
   "source": [
    "## Reading audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EEZPraFGZbbh"
   },
   "source": [
    "Digital audio is a discrete representation of the original mechanical waveform which is continuous in both the time and amplitude domains. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cats_dogs_audio.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
